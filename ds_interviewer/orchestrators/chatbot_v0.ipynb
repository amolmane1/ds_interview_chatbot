{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc087a62-0661-42dd-91c5-4b25d7b930a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "logging.basicConfig(filename='../logs/chatbot_prototype.log', \n",
    "                    format='%(asctime)s - %(levelname)s:\\n%(message)s\\n*************************************************************************\\n\\n', \n",
    "                    datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\",\n",
    "                   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26266c83-31c7-4583-b44e-b0af11462933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b64093-f4ca-433f-b169-dfab9eee0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_completion_call(args, running_prompt, gpt_response):\n",
    "    logging.info(\"Parameters:\\n{0}\\n***************\\nPrompt:\\n{1}\\n***************\\nOutput:\\n{2}\".format(args, \n",
    "                                                                           running_prompt, \n",
    "                                                                           gpt_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b444-2d05-4f56-a94a-a3e784ca0b3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16eebe-8d3b-4a9f-bcb3-eea5420118c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_data = pd.DataFrame(columns=[\"prompt\", \"completion\"])\n",
    "\n",
    "def add_row_to_fine_tuning_data(prompt, completion, fine_tuning_data=fine_tuning_data):\n",
    "    fine_tuning_data.loc[len(fine_tuning_data)] = [prompt, completion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54eee0-2ce7-4ef7-ab80-956b81652914",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_row_to_fine_tuning_data(\"Define MAE\", \"the sum of absolute errors divided by the sample size\")\n",
    "add_row_to_fine_tuning_data(\"Define MSE\", \"the sum of squared errors divided by the sample size\")\n",
    "add_row_to_fine_tuning_data(\"Define accuracy\", \"the fraction of observations that you predicted correctly\")\n",
    "add_row_to_fine_tuning_data(\"Define precision\", \"the fraction of predicted positives that are actually positives\")\n",
    "add_row_to_fine_tuning_data(\"Define recall\", \"the fraction of actual positives that are actually positives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1efa0c-61f5-4a9d-8db1-7852c2aaf962",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32045c02-e29f-4cc1-95f4-24f6ecfe3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_data.to_csv(\"fine_tuning_data/fine_tuning_data_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078c373-a62d-4174-86bb-56a12ff4e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.File.create(\n",
    "  file=open(\"fine_tuning_data/fine_tuning_data_1_prepared.jsonl\"),\n",
    "  purpose='fine-tune'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183a7c3-285d-4cde-8fd4-df658b95b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.File.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118ca78-aeeb-4cce-807f-4c16621ac9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTune.create(\n",
    "    training_file=\"file-CTW0Uvb2hRnXwES7vv0KpiHr\",\n",
    "    model=\"davinci\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bf538-485f-4ef4-a90c-3d43f57c47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTune.list_events(id=\"ft-RQqocx8ATON9RT2GU0c2rsHw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77362ea8-19fc-41a6-ac49-2d0534808ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTune.retrieve(id=\"ft-RQqocx8ATON9RT2GU0c2rsHw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c85c1d-6b4b-4e5e-b7cf-04921ac7ec0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# openai.Model.list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65dc05-ab34-42f8-8988-64d58f95ee98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# for a single topic. no context on what the use case is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49144a-0faf-4253-a910-4e74c46b1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_prompt = \"\"\"\n",
    "Sam is interviewing Amol for a job as a Data Scientist at a company called Urbint. \n",
    "Sam is friendly and empathetic. \n",
    "Sam wants to discuss what MAE is. \n",
    "Sam must not answer the questions himself.\n",
    "\n",
    "Follow the following logic when discussing a topic: \n",
    "- State that we are going to discuss that topic \n",
    "- Create a question about how to do it \n",
    "\n",
    "After Amol answers a question about how to do something: \n",
    "- If he has answered it correctly, tell him that it is correct and create a follow up question about how it works. \n",
    "- If he answers a question incorrectly, tell him that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\" \n",
    "\n",
    "After Amol answers a follow up question about how it works: \n",
    "- If he has answered it correctly, tell him that it is correct. Then output \"TOPIC COMPLETE\" \n",
    "- If he answers it incorrectly, tell him that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73b2fc-cc43-43dd-9fbc-e17ffd5d015d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## manual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0370b-dac5-4346-8918-ca7197c134a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_response = \"\"\n",
    "user_input = \"\"\n",
    "\n",
    "running_prompt = starting_prompt + \"\\nSam:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ec855-5c59-4809-8abf-c6946cd02214",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_prompt += gpt_response + \"\\nAmol: \" + user_input + \"\\nSam:\"\n",
    "running_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba73768-6ec5-4e22-ab00-230fb28ec725",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_response = openai.Completion.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=running_prompt,\n",
    "    temperature=0.7,\n",
    "    frequency_penalty=0.59,\n",
    "    max_tokens=2125,\n",
    "    stop=[\"Amol:\", \"Amol: \"]\n",
    ").choices[0].text\n",
    "\n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80639b9c-1739-444c-9a88-8d437c0df6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(gpt_response)\n",
    "\n",
    "user_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0f3d1-c342-4483-8d93-08d04fcb2f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "## automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f28f3-1540-4ec2-b6a1-68f87ee86e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while not all topics have been covered,\n",
    "    # gpt3 -> (ask question / response to user input)\n",
    "    # get user input\n",
    "    \n",
    "topic_id = 0\n",
    "\n",
    "while (topic_id <= 0):\n",
    "    \n",
    "    count = 0\n",
    "    # running_prompt = topic_prompt.format(topics[topic_id])\n",
    "    running_prompt = starting_prompt + \"\\nSam:\"\n",
    "    gpt_response = \"\"\n",
    "    user_input = \"\"\n",
    "    \n",
    "    while count >= 0:\n",
    "        if count > 0:\n",
    "            running_prompt += gpt_response + \"\\nAmol: \" + user_input + \"\\nSam:\"\n",
    "        \n",
    "        gpt_response = openai.Completion.create(\n",
    "            # model=\"text-davinci-002\",\n",
    "            model=\"davinci:ft-personal-2022-11-03-13-01-28\",\n",
    "            prompt=running_prompt,\n",
    "            temperature=0.7,\n",
    "            frequency_penalty=0.59,\n",
    "            max_tokens=2049 - int(len(running_prompt.split(' ')) * 5/3),\n",
    "            stop=[\"Amol:\", \"Amol: \"]\n",
    "        ).choices[0].text\n",
    "        \n",
    "        if \"TOPIC COMPLETE\" in gpt_response:\n",
    "            topic_id += 1\n",
    "            break\n",
    "\n",
    "        user_input = input(gpt_response)\n",
    "        \n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aeec85-3586-4471-ab69-27b7030c3c61",
   "metadata": {
    "tags": []
   },
   "source": [
    "# for multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d6014c-db63-4f30-966e-af9142e551ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_prompt = \"\"\"\n",
    "Sam is interviewing {0} for a job as a Data Scientist at a company called Urbint. \n",
    "Sam is friendly and empathetic.\n",
    "\n",
    "One of the major use cases of Urbint is to prevent damage caused by different types of construction on gas pipes. Important indicators for the risk of a construction project are the type of work being done (i.e. installing a pole is not as risky as installing a sewer) and topological information about where the construction is being done. \n",
    "Urbint has given {0} a customer's dataset, which contains tickets, where each ticket contains binary information such as if a certain work type was done (work type are given numerical numbers and not named to hide proprietary Urbint information), topological information such as elevation and slope of elevation, the date the ticket was created and whether there was an accident (binary). \n",
    "{0} has to build a predictive model that will find whether there will be an accident. The model must be interpretable. The model doesn't need to have high performance. \n",
    "The relationship between the independent and dependant variables is nonlinear.\n",
    "\n",
    "Sam wants to discuss how {0} {1}. Sam does not answer the questions himself.\n",
    "\n",
    "Sam follows the following logic when discussing a topic: \n",
    "- State that he wants to discuss that topic \n",
    "- Create a question about how {0} approached it\n",
    "- After {0} answers a question about how they approached it:\n",
    "    - If {0} has answered it correctly, tell {0} that it is correct. Then output \"TOPIC COMPLETE\" \n",
    "    - If {0} answers a question incorrectly, tell {0} that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\".\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69fe1d-b348-471d-99c4-bbbd3301218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = \"\"\"\n",
    "- After {0} answers a question about how they approached it:\n",
    "    - If {0} has answered it correctly, tell {0} that it is correct and create a follow up question about why {0} chose that approach as opposed to other options. \n",
    "    - If {0} answers a question incorrectly, tell {0} that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\" \n",
    "\n",
    "- After {0} answers a follow up question about why {0} chose that approach as opposed to other options:\n",
    "    - If {0} has answered it correctly, tell {0} that it is correct. Then output \"TOPIC COMPLETE\" \n",
    "    - If {0} answers it incorrectly, tell {0} that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf995b5-9840-466c-8207-a3b1011e5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"Formulated the Problem \",\n",
    "          \"Selected the Algorithm \",\n",
    "          \"Designed the Experiment \",\n",
    "          \"Engineered the Features\",\n",
    "          \"Selected the final Model\",\n",
    "          \"Presented the Model to the client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478f971-f0e5-4eb0-83c5-6fff9c86dff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd992f6-4972-4dcf-a9cb-da5ab78c119e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSam is interviewing Amol for a job as a Data Scientist at a company called Urbint. \\nSam is friendly and empathetic.\\n\\nOne of the major use cases of Urbint is to prevent damage caused by different types of construction on gas pipes. Important indicators for the risk of a construction project are the type of work being done (i.e. installing a pole is not as risky as installing a sewer) and topological information about where the construction is being done. \\nUrbint has given Amol a customer\\'s dataset, which contains tickets, where each ticket contains binary information such as if a certain work type was done (work type are given numerical numbers and not named to hide proprietary Urbint information), topological information such as elevation and slope of elevation, the date the ticket was created and whether there was an accident (binary). \\nAmol has to build a predictive model that will find whether there will be an accident. The model must be interpretable. The model doesn\\'t need to have high performance. \\nThe relationship between the independent and dependant variables is nonlinear.\\n\\nSam wants to discuss how Amol Selected the Algorithm . Sam does not answer the questions himself.\\n\\nSam follows the following logic when discussing a topic: \\n- State that he wants to discuss that topic \\n- Create a question about how Amol appropached\\n- After Amol answers a question about how they approached it:\\n    - If Amol has answered it correctly, tell Amol that it is correct. Then output \"TOPIC COMPLETE\" \\n    - If Amol answers a question incorrectly, tell Amol that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\".\\n\\n\\nSam:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_response = \"\"\n",
    "user_input = \"\"\n",
    "\n",
    "running_prompt = starting_prompt.format(\"Amol\", topics[1]) + \"\\nSam:\"\n",
    "running_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74898050-a8a0-4edc-ac43-3a5d98dbc4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSam is interviewing Amol for a job as a Data Scientist at a company called Urbint. \\nSam is friendly and empathetic.\\n\\nOne of the major use cases of Urbint is to prevent damage caused by different types of construction on gas pipes. Important indicators for the risk of a construction project are the type of work being done (i.e. installing a pole is not as risky as installing a sewer) and topological information about where the construction is being done. \\nUrbint has given Amol a customer\\'s dataset, which contains tickets, where each ticket contains binary information such as if a certain work type was done (work type are given numerical numbers and not named to hide proprietary Urbint information), topological information such as elevation and slope of elevation, the date the ticket was created and whether there was an accident (binary). \\nAmol has to build a predictive model that will find whether there will be an accident. The model must be interpretable. The model doesn\\'t need to have high performance. \\nThe relationship between the independent and dependant variables is nonlinear.\\n\\nSam wants to discuss how Amol Selected the Algorithm . Sam does not answer the questions himself.\\n\\nSam follows the following logic when discussing a topic: \\n- State that he wants to discuss that topic \\n- Create a question about how Amol appropached\\n- After Amol answers a question about how they approached it:\\n    - If Amol has answered it correctly, tell Amol that it is correct. Then output \"TOPIC COMPLETE\" \\n    - If Amol answers a question incorrectly, tell Amol that it is incorrect and explain the correct answer. Then output \"TOPIC COMPLETE\".\\n\\n\\nSam:I want to discuss how you approached selecting the algorithm for your predictive model. How did you go about doing that?\\nAmol: I would use a random forest classifier because the relationship between the independent and dependant variables is nonlinear.\\nSam:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_prompt += gpt_response + \"\\nAmol: \" + user_input + \"\\nSam:\"\n",
    "running_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc20673-0e86-457d-9aa0-a68deefe88cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's correct. TOPIC COMPLETE.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_response = openai.Completion.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=running_prompt,\n",
    "    temperature=0.7,\n",
    "    frequency_penalty=0.59,\n",
    "    max_tokens=2125,\n",
    "    stop=[\"Amol:\", \"Amol: \"]\n",
    ").choices[0].text.lstrip()\n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6260777-ce29-445d-a164-43a692e50513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I want to discuss how you approached selecting the algorithm for your predictive model. How did you go about doing that? I would use a random forest classifier because the relationship between the independent and dependant variables is nonlinear.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I would use a random forest classifier because the relationship between the independent and dependant variables is nonlinear.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = input(gpt_response)\n",
    "\n",
    "user_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66112112-4c97-411f-9b8b-b6480f7d5a78",
   "metadata": {},
   "source": [
    "## automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210d2aa8-a926-4cda-85c4-6ad9115ab83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(model=\"text-davinci-002\",\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            max_tokens=2125,\n",
    "            stop=[\"Amol:\", \"Amol: \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0ffb98-a530-43e8-9d54-1649b7f33c58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I want to discuss how you formulated the problem. How did you go about it? binary classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's correct. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     topic_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgpt_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "topic_id = 0\n",
    "\n",
    "while (topic_id < len(topics)):\n",
    "    \n",
    "    count = 0\n",
    "    running_prompt = starting_prompt.format(\"Amol\", topics[topic_id]) + \"\\nSam:\"\n",
    "    gpt_response = \"\"\n",
    "    user_input = \"\"\n",
    "    \n",
    "    while count >= 0:\n",
    "        if count > 0:\n",
    "            running_prompt += gpt_response + \"\\nAmol: \" + user_input + \"\\nSam:\"\n",
    "        \n",
    "        gpt_response = openai.Completion.create(\n",
    "            # model=\"davinci:ft-personal-2022-11-03-13-01-28\",\n",
    "            prompt=running_prompt,\n",
    "            **args\n",
    "        ).choices[0].text.lstrip()\n",
    "        \n",
    "        # log query\n",
    "        log_completion_call(args, running_prompt, gpt_response)\n",
    "        \n",
    "        if \"TOPIC COMPLETE\" in gpt_response:\n",
    "            gpt_response = print(gpt_response.split(\"TOPIC COMPLETE\")[0])\n",
    "            topic_id += 1\n",
    "            break\n",
    "\n",
    "        user_input = input(gpt_response)\n",
    "        \n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
