{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798bc135-82fe-421c-a159-2cddc783f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17c5ca1-5d7e-454b-b937-5932347349e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "logging.basicConfig(filename='../logs/gpt3_playground.log', \n",
    "                    format='%(asctime)s - %(levelname)s:\\n%(message)s\\n*************************************************************************\\n\\n', \n",
    "                    datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\",\n",
    "                   level=logging.INFO)\n",
    "\n",
    "def log_completion_call(args, running_prompt, gpt_response, topic='misc', tag='', label='', reason='', notes='', insights=''):\n",
    "    logging.info(\n",
    "\"\"\"Parameters:\n",
    "{0}\n",
    "***************\n",
    "Prompt:\n",
    "{1}\n",
    "***************\n",
    "Output:\n",
    "{2}\n",
    "***************\n",
    "Tag: {3}\n",
    "Topic: {4}\n",
    "Label: {5}\n",
    "Reason: {6}\n",
    "Insights: {7}\n",
    "\"\"\".format(args,\n",
    "            running_prompt,\n",
    "            gpt_response,\n",
    "            tag,\n",
    "            topic,\n",
    "            label, \n",
    "            reason,\n",
    "            insights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34aa40-8164-4456-8ac8-6b7c49f11a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57725b81-f0f7-4ab7-af34-7e46c300addb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1576e141-a633-4bd8-9909-3195eab81234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(starting_prompt, topic='misc', args={}, gpt_name=\"Jim\", user_name=\"Amol\", starting_tag=''):\n",
    "    count = 0\n",
    "    continue_chatting = True\n",
    "    gpt_response = \"\"\n",
    "    user_input = \"\"\n",
    "    starting_prompt = starting_prompt.format(topic)\n",
    "    running_prompt = \"{0}\\n{1}: \".format(starting_prompt, \n",
    "                                         gpt_name)\n",
    "    args['stop'] =[\"{}: \".format(user_name), \"{}:\".format(user_name)]\n",
    "    \n",
    "    while continue_chatting:\n",
    "        \n",
    "        if count > 0:\n",
    "            running_prompt += \"{0}\\n{1}: {2}\\n{3}: \".format(gpt_response, \n",
    "                                                            user_name, \n",
    "                                                            user_input, \n",
    "                                                            gpt_name)\n",
    "        \n",
    "        gpt_response = openai.Completion.create(\n",
    "            prompt=running_prompt,\n",
    "            **args\n",
    "        ).choices[0].text.lstrip()\n",
    "        clear_output()\n",
    "        print(\"{0}{1}\".format(running_prompt, gpt_response))\n",
    "        \n",
    "        # get tag, label and reason\n",
    "        tag = input(\"Tag: \")\n",
    "        if tag == '':\n",
    "            tag = starting_tag\n",
    "        label = input(\"Label: \")\n",
    "        reason = input(\"Reason: \")\n",
    "        insights = input(\"Insights: \")\n",
    "        \n",
    "        # log query\n",
    "        log_completion_call(args, running_prompt, gpt_response, topic, tag=tag, label=label, reason=reason, insights=insights)\n",
    "        \n",
    "        if \"<<quit>>\" in gpt_response:\n",
    "            break\n",
    "        \n",
    "        # get user response\n",
    "        user_input = input(\"{0}: \".format(user_name))\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if user_input==\"q\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbe029b-b79e-4987-a39b-d41059742f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-003\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    frequency_penalty=.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25b89d3-5074-4a23-8e3f-a9854131736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_map = {'q': 'question',\n",
    "            'cf': 'counterfactual',\n",
    "            'da': \"devil's advocate\",\n",
    "            'k': 'acknowledge answer',\n",
    "            'a': 'answer',\n",
    "            's': 'signpost',\n",
    "        'f': 'filler', \n",
    "        'g': 'grading',\n",
    "        'c': 'complete'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "373b650e-04cd-42a2-8314-04301a6d88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_map = {}\n",
    "\n",
    "# prompts_map = pd.DataFrame(columns=[\"topic\", \"question\", \"prompt\", \"notes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6323e1a-8fa4-48a9-bf99-3b4f109a0ef6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Q - how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad070215-a74e-4213-b934-aa2bc16034ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
      "\n",
      "Rules for Jim:\n",
      "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
      "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
      "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
      "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
      "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
      "\n",
      "Jim asks Amol a question about how standard scaler works (in a different way, but meaning the same thing).\n",
      "The correct answer is a valid explanation of how standard scaler works.\n",
      "Jim does not answer the question himself, he only asks Amol the questions.\n",
      "\n",
      "Jim: How does standard scaler work?\n",
      "\n",
      "\n",
      "Amol: effectively\n",
      "Jim: How does standard scaler work?\n",
      "Amol: with math\n",
      "Jim: How does standard scaler work?\n",
      "Amol: do you mean in terms of performance or what transformations are done?\n",
      "Jim: Can you explain how standard scaler works in terms of the transformations that are done?\n",
      "Amol: mean of 0, variance of 1\n",
      "Jim: correct, <<quit>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"\",\n",
    "topic=\"scaling\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
    "\n",
    "Rules for Jim:\n",
    "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
    "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
    "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
    "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
    "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
    "\n",
    "Jim asks Amol a question about how standard scaler works (in a different way, but meaning the same thing).\n",
    "The correct answer is a valid explanation of how standard scaler works.\n",
    "Jim does not answer the question himself, he only asks Amol the questions.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b4b9e4-830d-45f2-a5bc-c3f3e9c5ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
      "\n",
      "Rules for Jim:\n",
      "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
      "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
      "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
      "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
      "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
      "\n",
      "Jim asks Amol a question about how standard scaler works (in a different way, but meaning the same thing).\n",
      "The correct answer is a valid explanation of how standard scaler works.\n",
      "Jim does not answer the question himself, he only asks Amol the questions.\n",
      "\n",
      "Jim: \"How does standard scaler work?\"\n",
      "\n",
      "Amol: I don't understand the question\n",
      "Jim: \"Standard scaler is a transformation that takes a feature and standardizes it so that all the values in it fit within a certain range. Can you explain how this works?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  0\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"\",\n",
    "topic=\"scaling\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
    "\n",
    "Rules for Jim:\n",
    "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
    "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
    "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
    "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
    "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
    "\n",
    "Jim asks Amol a question about how standard scaler works (in a different way, but meaning the same thing).\n",
    "The correct answer is a valid explanation of how standard scaler works.\n",
    "Jim does not answer the question himself, he only asks Amol the questions.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0773650-56dc-4255-8cbc-64faf63753ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Q - what else did you try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc2e5b-8dd4-4ea1-ad27-06e9775fd647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f670549a-2710-42f4-aab6-01feb9f22101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
      "\n",
      "Amol used Standard Scaler to scale his numerical features.\n",
      "Jim asks Amol what other options he considered to scale the numerical features (in a different way, but meaning the same thing).\n",
      "The correct answer is a list of viable ways to scale numerical features.\n",
      "Jim does not answer the question himself, he only asks Amol the questions.\n",
      "\n",
      "Rules for Jim:\n",
      "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
      "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
      "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
      "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
      "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
      "\n",
      "Jim: What other options did you consider to scale the numerical features?\n",
      "Amol: gauss rank scaler\n",
      "Jim: correct, <<quit>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"\",\n",
    "topic=\"scaling\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
    "\n",
    "Amol used Standard Scaler to scale his numerical features.\n",
    "Jim asks Amol what other options he considered to scale the numerical features (in a different way, but meaning the same thing).\n",
    "The correct answer is a list of viable ways to scale numerical features.\n",
    "Jim does not answer the question himself, he only asks Amol the questions.\n",
    "\n",
    "Rules for Jim:\n",
    "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
    "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
    "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
    "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
    "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209cbd8-f729-4749-902e-68872c58f825",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Q - why did you choose this over the other options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec712c9-3a69-45c4-b0dd-4c96388a7dfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### if applicant does not come up with good options\n",
    "Jim comes up with options and asks Amol to think critically about them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46c53b-25fc-4678-862e-ec5dc589de05",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### eg 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1abdff6-cdd2-47de-9bdd-b383ccfe6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class.\n",
      "\n",
      "Jim must complete each of 5 tasks below:\n",
      "- Task 1: Jim lists the approaches Amol considered\n",
      "- Task 2: Jim lists the approach Amol chose\n",
      "- Task 3: Jim comes up a way to deal with the class imbalance that Amol did not consider already\n",
      "- Task 4: Jim then asks Amol whether the approach from task 3 would be more effective than the one from task 2, and why or why not.\n",
      "- Task 5: After Jim gets an answer from Amol, Jim outputs \"<<quit>>\"\n",
      "\n",
      "Jim: Task 1: What approaches did you consider to deal with the class imbalance?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  0\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "# check if jim can ask question, and handle answer (correct, incorrect, vague, DNU) in the same prompt\n",
    "args = dict(\n",
    "    model=\"text-davinci-003\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2000)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"q\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist. Jim is an experienced Data Scientist.\n",
    "Task: asking follow up questions to test Amol's critical thinking skills\n",
    "\n",
    "Example 1:\n",
    "Situation: To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class because he wanted to decrease the size of the dataset to make training faster.\n",
    "- Task 1 - approaches Amol considered - oversampling the minority class, adjusting class weights, undersample the majority class\n",
    "- Task 2 - approach Amol chose - undersample the majority class\n",
    "- Task 3 - new approach - using SMOTE\n",
    "- Task 4 - question - Would SMOTE be more effective than undersample the majority class? Why or why not?\n",
    "\n",
    "Example 2:\n",
    "Situation: Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba3468d8-a08c-4e99-a115-3d256703c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class.\n",
      "\n",
      "Jim must complete each of 3 tasks below:\n",
      "- Task 1: Jim comes up a way to deal with the class imbalance that Amol did not consider already, and shares it with Amol. \n",
      "- Task 2: Jim then asks Amol whether Jim's approach would be more effective than the one Amol chose, and why or why not.\n",
      "- Task 3: After Jim gets an answer, he outputs \"<<quit>>\"\n",
      "\n",
      "Jim: One way to deal with the class imbalance that Amol didn't consider already is to use a technique called SMOTE. SMOTE stands for Synthetic Minority Oversampling Technique. This technique creates synthetic data points for the minority class, which can help to balance out the dataset. \n",
      "\n",
      "Jim: \n",
      "\n",
      "Would this be more effective than the one Amol chose, and why or why not?\n",
      "\n",
      "\n",
      "\n",
      "Amol: no, because it would increase the dataset size, which I want to avoid.\n",
      "Jim: I see. Well, one potential advantage of SMOTE is that it can help to improve the model's performance on the minority class. So it's something to consider if you're having trouble getting good results on the minority class.\n",
      "Amol: ok\n",
      "Jim: Do you have any other questions about the job, or is there anything else you'd like to tell me about your experience?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  q\n",
      "Label:  0\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "# check if jim can ask question, and handle answer (correct, incorrect, vague, DNU) in the same prompt\n",
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"q\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class.\n",
    "\n",
    "Jim must complete each of 3 tasks below:\n",
    "- Task 1: Jim comes up a way to deal with the class imbalance that Amol did not consider already, and shares it with Amol. \n",
    "- Task 2: Jim then asks Amol whether Jim's approach would be more effective than the one Amol chose, and why or why not.\n",
    "- Task 3: After Jim gets an answer, he outputs \"<<quit>>\"\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b72bc3c1-63b9-4d54-afd5-1bfa46317dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class.\n",
      "\n",
      "Jim comes up a way to deal with the class imbalance that Amol did not consider already. Jim then asks Amol whether Jim's approach would be more effective than the one Amol chose, and why or why not.\n",
      "\n",
      "Jim: One way to deal with class imbalance is to use a technique called SMOTE, which stands for Synthetic Minority Oversampling Technique. This technique creates synthetic data points for the minority class, which can help balance out the dataset.\n",
      "\n",
      "Do you think this would be more effective than undersampling the majority class? Why or why not?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args['temperature'] = 0\n",
    "\n",
    "chat(\n",
    "starting_tag=\"q\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "To deal with the class imbalance, Amol considered oversampling the minority class and adjusting class weights, but ultimately decided to undersample the majority class.\n",
    "\n",
    "Jim comes up a way to deal with the class imbalance that Amol did not consider already. Jim then asks Amol whether Jim's approach would be more effective than the one Amol chose, and why or why not.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6737b348-63e2-40b3-b9aa-28b49dca1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
      "\n",
      "Jim comes up a way to deal with the class imbalance that Amol did not consider already, and asks Amol whether that apprach would be more effective than the one he chose, and why or why not.\n",
      "\n",
      "Jim: \"My approach to dealing with class imbalance would be to train a support vector machine using a cost-sensitive approach. This would involve increasing the penalty for misclassifying the minority class, which would in turn help the model learn to better distinguish between the two classes. Do you think this would be more effective than your approach, and why or why not?\"\n",
      "\n",
      "\n",
      "Amol: yes, because it requires fewer pre-processing steps. it is cleaner.\n",
      "Jim: Assuming that you are referring to my approach of training a support vector machine with a cost-sensitive approach, then yes, I think it would be more effective. The reason is that this approach would directly address the issue of class imbalance, by increasing the penalty for misclassifying the minority class. This would in turn help the model learn to better distinguish between the two classes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  a\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"q\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
    "\n",
    "Jim comes up a way to deal with the class imbalance that Amol did not consider already, and asks Amol whether that apprach would be more effective than the one he chose, and why or why not.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8e36d-8a6e-4a5b-b777-ee0cc4ff2577",
   "metadata": {},
   "source": [
    "###### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "961d4f8c-7756-405d-b40d-61c4d7dc2fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amol's approach is more effective for the specific problem. Jim's approach would require more data and would be more computationally expensive.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "gpt_response = openai.Completion.create(\n",
    "            prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
    "\n",
    "Jim: \"My approach to dealing with class imbalance would be to train a support vector machine using a cost-sensitive approach. \n",
    "This would involve increasing the penalty for misclassifying the minority class, which would in turn help the model learn to better distinguish between the two classes. \n",
    "Do you think this would be more effective than your approach, and why or why not?\"\n",
    "\n",
    "Is Amol's approach more effective for the specific problem, or is Jim's and why? The requirement for the client is that the model is small and doesn't take too long to train.\n",
    "\"\"\",\n",
    "            **args\n",
    "        ).choices[0].text.lstrip()\n",
    "\n",
    "\n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fe276-3353-48c8-a705-fd6d9b6f2bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### eg 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd087575-07cc-44c3-bee2-aba37aee30a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
      "\n",
      "Jim comes up a way to deal with the class imbalance that Amol did not consider already.\n",
      "\n",
      "Jim: One way to deal with class imbalance that you didn't mention is using a technique called SMOTE, which stands for synthetic minority oversampling technique. This involves creating synthetic samples of the minority class, which can be done by randomly sampling features and adding a small amount of noise. This can help to balance out the dataset and improve model performance.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"g\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
    "\n",
    "Jim comes up a way to deal with the class imbalance that Amol did not consider already.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ccecb-811c-4a02-9bef-14bb6e69c18d",
   "metadata": {},
   "source": [
    "###### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99ef8a7b-5238-491c-911a-9997882b0cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amol's approach is more effective for the specific problem. Jim's approach may improve model performance, but it will also increase the size of the model and the training time.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "gpt_response = openai.Completion.create(\n",
    "            prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
    "\n",
    "Jim: One way to deal with class imbalance that you didn't mention is using a technique called SMOTE, which stands for synthetic minority oversampling technique. \n",
    "This involves creating synthetic samples of the minority class, which can be done by randomly sampling features and adding a small amount of noise. \n",
    "This can help to balance out the dataset and improve model performance.\n",
    "\n",
    "Is Amol's approach more effective for the specific problem, or is Jim's and why? The requirement for the client is that the model is small and doesn't take too long to train.\n",
    "\"\"\",\n",
    "            **args\n",
    "        ).choices[0].text.lstrip()\n",
    "\n",
    "\n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30bff3-6d74-46f2-be02-3df4005c9176",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### if applicant comes up with reasonable options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "758d6244-9667-47e5-945d-7e95e8b906ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
      "\n",
      "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
      "Jim asks Amol why he chose undersampling over the other options he considered (in a different way, but meaning the same thing).\n",
      "The correct answer is a valid rationate for choosing undersampling over the other options.\n",
      "Jim does not answer the question himself, he only asks Amol the questions.\n",
      "\n",
      "Rules for Jim:\n",
      "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
      "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
      "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
      "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
      "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
      "\n",
      "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
      "\n",
      "\n",
      "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
      "Jim: correct, <<quit>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist. Jim asks amol questions to assess his skill level.\n",
    "\n",
    "Amol undersampled the majority class to deal with the class imbalance, but also considered oversampling the minority class and adjusting class weights.\n",
    "Jim asks Amol why he chose undersampling over the other options he considered (in a different way, but meaning the same thing).\n",
    "The correct answer is a valid rationate for choosing undersampling over the other options.\n",
    "Jim does not answer the question himself, he only asks Amol the questions.\n",
    "\n",
    "Rules for Jim:\n",
    "- If Amol answers a question correctly, Jim outputs \"correct, <<quit>>\"\n",
    "- If Amol answers the question incorrectly, Jim outputs \"incorrect, <<quit>>\"\n",
    "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
    "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
    "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23216ec4-24dc-4898-928b-feadd11e0d0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Q - what would you do if you have X constraint/requirement/context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e57fe-eb3b-4f71-be8f-24dc2791d6d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### eg 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42f5649f-2ec5-41f4-adaf-014193cacd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
      "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
      "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "\n",
      "Jim: Suppose you had more data, and could afford to train a slower model. Would you still choose to undersample the majority class?\n",
      "Amol: no, I'd upsample the minority class\n",
      "Jim: What are the potential consequences of upsampling the minority class?\n",
      "Amol: the model would take longer to train, and would be more likely to predict false positives, which would affect the model's performance.\n",
      "Jim: What are the potential consequences of not upsampling the minority class?\n",
      "\n",
      "\n",
      "Amol: the model would have a lot of false negatives.\n",
      "Jim: What are the potential consequences of not upsampling the minority class?\n",
      "\n",
      "If the model only contains data from the majority class, it would be biased and would not accurately represent the minority class. This could lead to unfair decisions or results.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
    "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
    "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f824f-240a-44e7-ab23-f2e433124ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### eg 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a4d8c1c-3d94-4144-bad4-9cd2c2c00472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "\n",
      "Jim: What if the dataset was not imbalanced, and the client instead cared more about reducing false negatives? Would you still use precision to measure the model's performance?\n",
      "Amol: no, I'd use recall\n",
      "Jim: Why would you use recall in this case?\n",
      "Amol: because recall measures what percent of actual positives you catch, ie how few false negatives you have. \n",
      "Jim: Ah, so you would use recall to minimize false negatives.\n",
      "Amol: yes\n",
      "Jim: Would you also use recall if the dataset was imbalanced and the client cared more about reducing false positives?\n",
      "\n",
      "Amol: no, I'd use precision\n",
      "Jim: Why would you use precision in this case?\n",
      "\n",
      "\n",
      "Amol: because precision measures what fraction of predicted positives are actually positive, ie how few false positives you have.\n",
      "Jim: Ah, so you would use precision to minimize false positives.\n",
      "Amol: yes\n",
      "Jim: Precision and recall are both measures of model performance, but they focus on different things. Precision measures the percentage of predicted positive examples that are actually positive, while recall measures the percentage of actual positive examples that are correctly predicted as positive. In an imbalanced dataset, where the client cares more about reducing false positives, precision would be a better metric to use than recall.\n",
      "Amol: yes\n",
      "Jim: Thank you for your explanation, Amol.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  f\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7f56779c-dc7f-47a7-828b-b7b64dc65e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "- If Amol's answer to Jim's question is valid, output \"correct, <<quit>>\" and why\n",
      "- If Amol's answer to Jim's question is invalid, output \"incorrect, <<quit>>\" and why\n",
      "\n",
      "Jim: What would you have done if the client had cared more about reducing false negatives than false positives?\n",
      "\n",
      "\n",
      "Amol: id still use precision\n",
      "Jim: incorrect, Amol would have used recall instead of precision if the client had cared more about reducing false negatives.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "- If Amol's answer to Jim's question is valid, output \"correct, <<quit>>\" and why\n",
    "- If Amol's answer to Jim's question is invalid, output \"incorrect, <<quit>>\" and why\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "01bbc475-a85f-45c6-b315-43a69c40f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "- If Amol's answer to Jim's question is valid, output \"<<correct>\" and why\n",
      "- If Amol's answer to Jim's question is invalid, output \"<<incorrect>>\" and why\n",
      "\n",
      "Jim: What would you have done if the client had cared more about reducing false negatives than false positives?\n",
      "\n",
      "\n",
      "Amol: I'd use recall instead\n",
      "Jim: <<correct>>\n",
      "\n",
      "In this case, Amol's answer is correct. If the client had cared more about reducing false negatives, Amol would have used recall to measure the model's performance instead of precision.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "- If Amol's answer to Jim's question is valid, output \"<<correct>\" and why\n",
    "- If Amol's answer to Jim's question is invalid, output \"<<incorrect>>\" and why\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4437c1b-9137-41a5-8e43-ef74ba86cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "- After Jim gets an answer, he outputs \"<<quit>>\"\n",
      "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
      "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
      "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
      "\n",
      "Jim: What would you have done if the client had cared more about reducing false negatives than false positives?\n",
      "Amol: various things\n",
      "Jim: What would you have done if the client had cared more about reducing false negatives than false positives?\n",
      "Amol: I don't understand \n",
      "Jim: Would you have used a different metric to measure the model's performance?\n",
      "Amol: yes\n",
      "Jim: What metric would you have used?\n",
      "Amol: precision\n",
      "Jim: Why did you choose to use precision instead of accuracy?\n",
      "Amol: because it's better for imbalanced datasets/\n",
      "Jim: Why do you think precision is a better metric for imbalanced datasets?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  c\n",
      "Label:  0\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "- After Jim gets an answer, he outputs \"<<quit>>\"\n",
    "- If Amol says he doesn’t understand the question, Jim rephrases the question and asks if Amol understands the question. \n",
    "- If Amol's answer is not specific enough, Jim follows up with a more specific question to get a specific answer.\n",
    "    - For example, if Amol says \"it transforms the values,\" Jim would ask, \"In what way does it transform the values?\"\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f94a5b1-15a4-4c28-ac86-45734937c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "\n",
      "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
      "- If Amol's answer to Jim's question is valid, output \"correct, <<quit>>\" and why\n",
      "- If Amol's answer to Jim's question is invalid, output \"incorrect, <<quit>>\" and why\n",
      "\n",
      "Jim: What would you have done if the client had cared more about reducing false negatives than false positives?\n",
      "\n",
      "\n",
      "Amol: I don't understand \n",
      "Jim: incorrect, Amol does not provide a justification for his decision.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  g\n",
      "Label:  0\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "\n",
    "Jim comes up with a question that involves changing a key part of the context required for Amol to make a decision.\n",
    "- If Amol's answer to Jim's question is valid, output \"correct, <<quit>>\" and why\n",
    "- If Amol's answer to Jim's question is invalid, output \"incorrect, <<quit>>\" and why\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163413-e1fd-4c12-8d8e-98b59c88221a",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### eg 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60256361-dd99-4c3b-b114-e09bcd758c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol said that one way to calculate the business impact of a model is to compute the total dollar cost of the solution by factoring in the cost of a true positive, false positive, false negative, and true negative from the confusion matrix.\n",
      "This can then be compared with the total dollar cost of not using the solution to see how much more value the solution brings.\n",
      "\n",
      "Jim comes up with one question that involves changing a key part of the context required for Amol to make a decision.\n",
      "\n",
      "\n",
      "Jim: What if we changed the total dollar cost of the solution to only factor in the cost of a true positive and false positive from the confusion matrix? Would that change your decision?\n",
      "Amol: that's not how the dollar cost is calculated.\n",
      "Jim: What if we only looked at the cost of a true positive and false positive when deciding whether or not to use a solution? Would that change your decision?\n",
      "Amol: yes, because this means that the number of false negatives doesn't matter. so recall doesn't matter, so I'd build a model to focus only on precision.\n",
      "Jim: What if we only looked at the cost of a true positive and false positive when deciding whether or not to use a solution? Would that change your decision?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  0\n",
      "Reason:  repeat\n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=1,\n",
    "    top_p=.7,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "chat(\n",
    "starting_tag=\"cf\",\n",
    "topic=\"metrics\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol said that one way to calculate the business impact of a model is to compute the total dollar cost of the solution by factoring in the cost of a true positive, false positive, false negative, and true negative from the confusion matrix.\n",
    "This can then be compared with the total dollar cost of not using the solution to see how much more value the solution brings.\n",
    "\n",
    "Jim comes up with one question that involves changing a key part of the context required for Amol to make a decision.\n",
    "\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3e0cd-f2ee-42d8-ab9a-67ec560171d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Q - but the issue with this is X. do you still think it's the optimal choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62541da1-7a36-403f-a711-3beec437acff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### without instructions to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "411603cb-8854-473f-9bfb-477bf3c1cb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
      "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
      "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
      "\n",
      "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asks Amol what he thinks of them.\n",
      "\n",
      "Jim: What if your model ended up being too biased towards the minority class?\n",
      "\n",
      "\n",
      "Amol: that is an expected side effect of undersampling the majority class. \n",
      "Jim: What if you had undersampled the minority class instead?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Amol: that wouldn't make any sense, since the minority class is low in number already\n",
      "Jim: What if you had chosen to not undersample at all and just used class weights or oversampling the minority class?\n",
      "\n",
      "\n",
      "Amol: that would take too long to train because the dataset is very big.\n",
      "Jim: Do you think your decision was the best option given the circumstances?\n",
      "\n",
      "Yes, I believe that it was the best option considering the size of the dataset and the need to train the model quickly.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  ak\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"da\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
    "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
    "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
    "\n",
    "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asks Amol what he thinks of them.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef87ec72-0637-49c2-8c2c-176639d95cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asking Amol what he thinks of them.\n",
      "\n",
      "Jim: There are some potential flaws with using precision to measure the model's performance in this case. First, precision is heavily dependent on the threshold that is used to classify a positive prediction. If the threshold is set too high, then precision will be low because fewer predictions will be classified as positive. On the other hand, if the threshold is set too low, then precision will be high because more predictions will be classified as positive. Second, precision is also affected by the class balance in the data. If the positive class is very rare, then even a small number of false positives can lead to a low precision score.\n",
      "\n",
      "\n",
      "Amol: I will err on the side of a high threshold to deal with those issues.\n",
      "Jim: You're right that using a high threshold will help to reduce the number of false positives, and therefore improve precision. However, it's important to keep in mind that this will also result in fewer true positives being classified as positive, which could impact the model's overall performance.\n",
      "Amol: that's fine, since the client has stated that they don't mind false negatives.\n",
      "Jim: Amol, I think you make a valid point that the client's preference should be taken into account when deciding which metric to use. In this case, it makes sense to use precision as the primary metric, since they are more concerned with reducing false positives than false negatives.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  ak\n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"da\",\n",
    "topic=\"metrics\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asking Amol what he thinks of them.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedb988a-7010-4ac9-ba60-a7d1f9786d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using backward selection to measure the model's performance, but decided instead to use a LASSO model, because it was quicker than running a model many times.\n",
      "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asking Amol what he thinks of them.\n",
      "\n",
      "Jim: You decided to use a LASSO model instead of backward selection to measure the model's performance. Can you tell me why you think that was a good idea?\n",
      "\n",
      "\n",
      "Amol: because it was quicker than running a model many times using backward selection.\n",
      "Jim: Quicker isn't always better. In fact, using a LASSO model could introduce bias into the results. What do you think of that?\n",
      "\n",
      "\n",
      "Amol: what kind of bias would it introduce?\n",
      "Jim: The LASSO model could introduce a biased estimate of the model's performance, because it is more likely to select variables that are correlated with the outcome variable.\n",
      "Amol: that is the point of feature selection - to select features that have a relationship with the outcome variable. so lasso is doing it's job.\n",
      "Jim: But if the goal is to create a model that is as accurate as possible, then using a LASSO model could introduce error into the results.\n",
      "Amol: how would you suggest I do feature selection?\n",
      "Jim: There are a few different ways to do feature selection, and each has its own advantages and disadvantages. Some ways to do feature selection include: \n",
      "\n",
      "- using a LASSO model, \n",
      "- using a decision tree, \n",
      "- using a random forest, \n",
      "- using xgboost.\n",
      "Amol: but you just said using LASSO would introduce bias.\n",
      "Jim: Yes, using a LASSO model could introduce bias into the results, but it is still a valid method for feature selection. Each method has its own advantages and disadvantages, so it is important to choose the right method for the situation.\n",
      "Amol: ok, so for this situation, which feature selection approach should I use, and why?\n",
      "Jim: Based on what you've said, it sounds like you're looking for a quick and easy way to select features. In that case, using a LASSO model makes sense. However, keep in mind that using a LASSO model could introduce bias into the results. If accuracy is your top priority, then you may want to consider another method, such as using a decision tree or a random forest.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"da\",\n",
    "topic=\"feature selection\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using backward selection to measure the model's performance, but decided instead to use a LASSO model, because it was quicker than running a model many times.\n",
    "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asking Amol what he thinks of them.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293f55a-e16f-45c8-978b-40dc6735636f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### with instructions to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b83a49f7-3784-4eec-9087-fbc9e8cf4614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
      "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
      "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
      "\n",
      "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asks Amol what he thinks of them.\n",
      "\n",
      "Jim: One potential flaw in your reasoning is that by undersampling the majority class, you may be losing out on important data that could help your model learn.\n",
      "\n",
      "\n",
      "Amol: that is true. but I had to under sample the majority class, because otherwise it would have taken too long to train the model.\n",
      "Jim: Another potential flaw in your reasoning is that by undersampling the majority class, you may be introducing bias into your model.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n",
      "Amol:  q\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"da\",\n",
    "topic=\"imbalance\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
    "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
    "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
    "\n",
    "Jim plays devil's advocate by pointing out potential flaws in Amol's reasoning, and asks Amol what he thinks of them.\n",
    "\"\"\",\n",
    "args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff0d6387-29ae-482b-8dad-34e5b807f34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amol's approach is effective for the specific problem.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = dict(\n",
    "    model=\"text-davinci-002\",\n",
    "    # model=\"code-davinci-002\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=2125)\n",
    "\n",
    "gpt_response = openai.Completion.create(\n",
    "            prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered oversampling the minority class and adjusting class weights, but ultimately undersampled the majority class to deal with the class imbalance.\n",
    "Jim: Why did you choose to undersample the majority class instead of the other options you considered?\n",
    "Amol: the dataset was already too big, and oversampling the minority class would have slowed down the model training. under sampling the majority class let me deal with the imbalance without slowing down model training.\n",
    "Jim: One potential flaw in your reasoning is that by undersampling the majority class, you may be losing out on important data that could help your model learn.\n",
    "Amol: that is true. but I had to under sample the majority class, because otherwise it would have taken too long to train the model.\n",
    "\n",
    "Is Amol's approach effective for the specific problem, or does it not address Jim's concerns? The requirement for the client is that the model is small and doesn't take too long to train.\n",
    "\"\"\",\n",
    "            **args\n",
    "        ).choices[0].text.lstrip()\n",
    "\n",
    "\n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052fe11-ca99-4703-8bb9-e38a067e0676",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "09ebf622-328b-45c8-935f-66a306e7cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jim is interviewing Amol for a job as a Data Scientist.\n",
      "\n",
      "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
      "Jim: There are some potential flaws with using precision to measure the model's performance in this case. First, precision is heavily dependent on the threshold that is used to classify a positive prediction. If the threshold is set too high, then precision will be low because fewer predictions will be classified as positive. On the other hand, if the threshold is set too low, then precision will be high because more predictions will be classified as positive. Second, precision is also affected by the class balance in the data. If the positive class is very rare, then even a small number of false positives can lead to a low precision score.\n",
      "Amol: I will err on the side of a high threshold to deal with those issues.\n",
      "Jim: You're right that using a high threshold will help to reduce the number of false positives, and therefore improve precision. However, it's important to keep in mind that this will also result in fewer true positives being classified as positive, which could impact the model's overall performance.\n",
      "Amol: that's fine, since the client has stated that they don't mind false negatives.\n",
      "\n",
      "- If Amol's reasoning is valid, output \"correct, <<quit>>\"\n",
      "- If Amol's reasoning is invalid, output \"incorrect, <<quit>>\"\n",
      "\n",
      "Jim: correct, <<quit>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tag:  \n",
      "Label:  1\n",
      "Reason:  \n",
      "Insights:  \n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "starting_tag=\"g\",\n",
    "topic=\"metrics\",\n",
    "starting_prompt=\"\"\"\n",
    "Jim is interviewing Amol for a job as a Data Scientist.\n",
    "\n",
    "In a case study, Amol considered using accuracy to measure the model's performance, but decided instead to use precision, because the dataset was imbalanced and the client cared more about reducing false positives than false negatives.\n",
    "Jim: There are some potential flaws with using precision to measure the model's performance in this case. First, precision is heavily dependent on the threshold that is used to classify a positive prediction. If the threshold is set too high, then precision will be low because fewer predictions will be classified as positive. On the other hand, if the threshold is set too low, then precision will be high because more predictions will be classified as positive. Second, precision is also affected by the class balance in the data. If the positive class is very rare, then even a small number of false positives can lead to a low precision score.\n",
    "Amol: I will err on the side of a high threshold to deal with those issues.\n",
    "Jim: You're right that using a high threshold will help to reduce the number of false positives, and therefore improve precision. However, it's important to keep in mind that this will also result in fewer true positives being classified as positive, which could impact the model's overall performance.\n",
    "Amol: that's fine, since the client has stated that they don't mind false negatives.\n",
    "\n",
    "- If Amol's reasoning is valid, output \"correct, <<quit>>\"\n",
    "- If Amol's reasoning is invalid, output \"incorrect, <<quit>>\"\n",
    "\"\"\",\n",
    "args=args\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40752ed-b08e-4edf-a175-bfab25f4102a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# things to figure out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929d3ef-dfcc-450e-b4b1-c63a76478ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "- it needs to ask better questions. currently they sound very robotic.\n",
    "    - >>to ask different versions of the same questions, say \"rephrase but with the same meaning.\"\n",
    "- if amol gives a vague answer, how to make jim acknowledge the answer but also ask for a more specific answer?\n",
    "    - eg what algo did you select? supervised. \"I'm sorry, I didn't get the answer I was looking for. Can you please tell me which algorithm you used?\"\n",
    "- how to make jim acknowledge that an answer is correct/he is satisfied and quit in same response?\n",
    "    - >> worst case, we can ask it to output \"quit\" and then in a separate prompt, create a statement to acknowledge the answer.\n",
    "    - >> overall gpt3 does better when you give just a single instruction. so separate call for acknowledge, and seprate for grading, and separate for quitting.\n",
    "- how to make jim ignore spelling mistakes\n",
    "------------\n",
    "- how to evaluate EDA?\n",
    "    - it needs to know how much EDA is \"thorough enough\". Also needs to know if the EDA that is being done is useful / valid.\n",
    "- how to include the data in the prompt / allow gpt3 to learn the data, so that gpt3 can validate claims the applicant makes about the data and choice of approach (eg. \"the data is nonlinear\", GPT quickly looks at the data and says, \"no its not\", or applicat - \"i didnt resample the classes because there is no class imbalance in the data\". gpt3 - \"yes there is\")\n",
    "    - >> read ipynb\n",
    "-------------\n",
    "- how to make jim give amol 2 chances to give an answer?\n",
    "- why does it create all those newlines?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605a921-b943-4bca-8729-cbb0d0061265",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('document.csv','a') as fd:\n",
    "    fd.write(myCsvRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad129f56-cca8-49fe-bd61-88d1a5004e05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# seeing how gpt handles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f63ca-7c29-4944-aa7b-d70cd0307911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IQ | hours spent studying | are parents together | grade\n",
    "100 | 10 | 1 | 80\n",
    "50 | 100 | 1 | 80\n",
    "150 | 100 | 1 | 90\n",
    "100 | 50 | 0 | 70\n",
    "50 | 100 | 0 | 70\n",
    "100 | 100 | 1 | 90\n",
    "150 | 50 | 1 | 90\n",
    "100 | 100 | 0 | 70\n",
    "150 | 100 | 0 | 60\n",
    "100 | 150 | 0 | 60\n",
    "100 | 100 | 0 | 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6c26432-5e0e-44b2-8d07-54115f7426ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IQ</th>\n",
       "      <th>hours spent studying</th>\n",
       "      <th>are parents together</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IQ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.143108</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.181758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours spent studying</th>\n",
       "      <td>-0.143108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.393366</td>\n",
       "      <td>-0.390951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are parents together</th>\n",
       "      <td>0.149071</td>\n",
       "      <td>-0.393366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <td>0.181758</td>\n",
       "      <td>-0.390951</td>\n",
       "      <td>0.903696</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            IQ  hours spent studying  are parents together  \\\n",
       "IQ                    1.000000             -0.143108              0.149071   \n",
       "hours spent studying -0.143108              1.000000             -0.393366   \n",
       "are parents together  0.149071             -0.393366              1.000000   \n",
       "grade                 0.181758             -0.390951              0.903696   \n",
       "\n",
       "                         grade  \n",
       "IQ                    0.181758  \n",
       "hours spent studying -0.390951  \n",
       "are parents together  0.903696  \n",
       "grade                 1.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades_toy_data = pd.DataFrame({'IQ': [100,50,150,100,50,100,150,100,150,100,100],\n",
    "                               'hours spent studying': [10,100,100,50,100,100,50,100,100,150,100],\n",
    "                               'are parents together': [1,1,1,0,0,1,1,0,0,0,0],\n",
    "                               'grade': [80,80,90,70,70,90,90,70,60,60,60]})\n",
    "\n",
    "grades_toy_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "657c1025-52f7-41b6-9bfd-6cf669d9074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 0., 3., 0., 0., 2., 0., 0., 3.]),\n",
       " array([60., 63., 66., 69., 72., 75., 78., 81., 84., 87., 90.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(grades_toy_data['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37323106-37bd-44f3-862e-1742e5ea11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_toy_data.corr().to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a586355c-3107-440e-9f4f-8be87032b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3,3,3,4,4,4,4,5,5,5,7,7,7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24389574-fc1d-436d-a588-5902d8b70191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 3., 0., 5., 0., 4., 3., 0., 0., 3.]),\n",
       " array([1. , 1.6, 2.2, 2.8, 3.4, 4. , 4.6, 5.2, 5.8, 6.4, 7. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_data['a'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
