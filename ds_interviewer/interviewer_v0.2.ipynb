{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a553a3e-5f86-4848-8074-6f304dd73cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from parse import parse\n",
    "\n",
    "from finetuning import prepare_data\n",
    "from utils import graph\n",
    "from utils import utils\n",
    "from nodes.node_functions import *\n",
    "from utils.models_metadata import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f329fa-0590-469c-9630-13619bf6498b",
   "metadata": {},
   "source": [
    "# playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b9e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"validate_why_applicant_picked_X_over_Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e29ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt_template', 'completion_template', 'nested_completion_templates', 'kshot_header', 'finetuned_model_name', 'stop_sequence', 'model_name', 'model_version'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b67850aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_args = dict(validation_of_response=\"1\",\n",
    "    applicant_justifications=\"\"\"Subpoint: <Binary Encoding requires fewer columns to encode a categorical variable than One Hot Encoding.><1><NA>\n",
    "Subpoint: <Tree-based models perform better with fewer columns.><1><NA>\"\"\",\n",
    "    num_correct_subpoints=\"2\", \n",
    "    num_incorrect_subpoints=\"0\", \n",
    "    num_irrelevant_subpoints=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f84588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of response: 1\n",
      "Applicant justifications:\n",
      "Subpoint: <Binary Encoding requires fewer columns to encode a categorical variable than One Hot Encoding.><1><NA>\n",
      "Subpoint: <Tree-based models perform better with fewer columns.><1><NA>\n",
      "Number of correct, incorrect, irrelevant subpoints: 2, 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb415acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_of_response': '1',\n",
       " 'applicant_justifications': [{'subpoint': 'Binary Encoding requires fewer columns to encode a categorical variable than One Hot Encoding.',\n",
       "   'validation_of_subpoint': '1',\n",
       "   'reason_for_validation': 'NA'},\n",
       "  {'subpoint': 'Tree-based models perform better with fewer columns.',\n",
       "   'validation_of_subpoint': '1',\n",
       "   'reason_for_validation': 'NA'}],\n",
       " 'num_correct_subpoints': '2',\n",
       " 'num_incorrect_subpoints': '0',\n",
       " 'num_irrelevant_subpoints': '0'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = utils.parse_completion_args(completion, model_metadata)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850f3782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of response: 1\n",
      "Applicant justifications:\n",
      "Subpoint: <Binary Encoding requires fewer columns to encode a categorical variable than One Hot Encoding.><1><NA>\n",
      "Subpoint: <Tree-based models perform better with fewer columns.><1><NA>\n",
      "Number of correct, incorrect, irrelevant subpoints: 2, 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reconstructed_completion = utils.prepare_completion_using_nested_args(res, model_metadata)\n",
    "print(reconstructed_completion)\n",
    "assert completion == reconstructed_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a615fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Interviewer: Thank you for taking the time out to interview with us. \\n    In this interview, we will cover a range of topics and ask questions to assess your skill level as a Data Scientist. \\n    If you don't understand a question, feel free to ask questions to get clarification.\\n    Let's get started.\"],\n",
       " ['Interviewer: Could you confirm whether you used a Generative Adversarial Network for this task?',\n",
       "  'Applicant: Yes, I used a Generative Adversarial Network for this task.',\n",
       "  'Interviewer: How does a Generative Adversarial Network work?',\n",
       "  'Applicant: GANs are deep learning models that produce fake data.',\n",
       "  'Interviewer: Besides GANs, did you consider any other algorithms for this task?',\n",
       "  'Applicant: Yes, I considered using autoencoders and also CNNs.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_by_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434706bf-9c2d-42bc-8494-82d193b7e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************\n",
      "********* Model Name *********\n",
      "validate_why_applicant_picked_X_over_Y\n",
      "********* Prompt *********\n",
      "Context:\n",
      "What interviewer thinks applicant did:\n",
      "{'algorithm selection': 'GAN', 'categorical encoding': 'hash encoding', 'handling numerical variables': 'robust scaler'}\n",
      "\n",
      "Objectives/Contraints:\n",
      "- Maximize model performance.\n",
      "- Model interpretability is not a strong requirement.\n",
      "\n",
      "\n",
      "Current section chat:\n",
      "Interviewer: Could you confirm whether you used a Generative Adversarial Network for this task?\n",
      "Applicant: Yes, I used a Generative Adversarial Network for this task.\n",
      "Interviewer: How does a Generative Adversarial Network work?\n",
      "Applicant: GANs are deep learning models that produce fake data.\n",
      "Interviewer: Besides GANs, did you consider any other algorithms for this task?\n",
      "Applicant: Yes, I considered using autoencoders and also CNNs.\n",
      "\n",
      "Details:\n",
      "Is validation correct: 1\n",
      "###\n",
      "\n",
      "********* Completion *********\n",
      "Validation of response: 1\n",
      "Applicant justifications:\n",
      "[{'subpoint': 'GANs are deep learning models that produce fake data.', 'validation_of_subpoint': '1', 'reason_for_validation': 'NA'}, {'subpoint': 'Autoencoders and CNNs were considered for this task.', 'validation_of_subpoint': '1', 'reason_for_validation': 'NA'}]\n",
      "Number of correct, incorrect, irrelevant subpoints: 2, 0, 0\n",
      "******************\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m current_node_function_args[\u001b[39m'\u001b[39m\u001b[39mvalidate_async\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m current_node_function_args[\u001b[39m'\u001b[39m\u001b[39mchat_history_by_section\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m chat_history_by_section\n\u001b[0;32m---> 11\u001b[0m current_node_output \u001b[39m=\u001b[39m current_node[\u001b[39m'\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m'\u001b[39;49m](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcurrent_node_function_args)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/nodes/node_functions.py:367\u001b[0m, in \u001b[0;36mvalidate_why_applicant_picked_X_over_Y\u001b[0;34m(carryover_data, validate_async, chat_history_by_section, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m completion_args \u001b[39m=\u001b[39m parse_completion_args(completion, model_metadata)\n\u001b[1;32m    359\u001b[0m observation_details \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(model_name\u001b[39m=\u001b[39mmodel_name, \n\u001b[1;32m    360\u001b[0m                            model_version\u001b[39m=\u001b[39mmodel_version,\n\u001b[1;32m    361\u001b[0m                            prompt_template\u001b[39m=\u001b[39mmodel_metadata[\u001b[39m'\u001b[39m\u001b[39mprompt_template\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m                            prompt \u001b[39m=\u001b[39m observation_prompt,\n\u001b[1;32m    366\u001b[0m                            completion\u001b[39m=\u001b[39mcompletion)\n\u001b[0;32m--> 367\u001b[0m completion_args_to_use_for_interview \u001b[39m=\u001b[39m submit_observation_for_finetuning_validation(observation_details, \n\u001b[1;32m    368\u001b[0m                                                                                     validate_async\u001b[39m=\u001b[39;49mvalidate_async)\n\u001b[1;32m    370\u001b[0m routing_value \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(completion_args_to_use_for_interview[\u001b[39m'\u001b[39m\u001b[39mvalidation_of_response\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(routing_value\u001b[39m=\u001b[39mrouting_value, \n\u001b[1;32m    372\u001b[0m             new_chat_lines\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:195\u001b[0m, in \u001b[0;36msubmit_observation_for_finetuning_validation\u001b[0;34m(observation_details, validate_async)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m# TODO: save queue to file before validating live and then load again, in case an error occurs and kills the program.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m validate_async:\n\u001b[0;32m--> 195\u001b[0m     result \u001b[39m=\u001b[39m validate_observation_for_finetuning(observation_details)\n\u001b[1;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mvalidated\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    197\u001b[0m         finetuning_validation_queue\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:159\u001b[0m, in \u001b[0;36mvalidate_observation_for_finetuning\u001b[0;34m(observation_details)\u001b[0m\n\u001b[1;32m    156\u001b[0m negative_observation_details_for_finetuning[\u001b[39m'\u001b[39m\u001b[39mprompt_args\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mis_completion_correct\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    157\u001b[0m negative_observation_details_for_finetuning[\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m negative_observation_details_for_finetuning[\u001b[39m'\u001b[39m\u001b[39mprompt_template\u001b[39m\u001b[39m'\u001b[39m] \\\n\u001b[1;32m    158\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnegative_observation_details_for_finetuning[\u001b[39m'\u001b[39m\u001b[39mprompt_args\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 159\u001b[0m add_observation_to_finetuning_datasets(negative_observation_details_for_finetuning)\n\u001b[1;32m    161\u001b[0m \u001b[39m# create positive observation, send to master data set\u001b[39;00m\n\u001b[1;32m    162\u001b[0m positive_observation_details_for_finetuning \u001b[39m=\u001b[39m deepcopy(observation_details)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:121\u001b[0m, in \u001b[0;36madd_observation_to_finetuning_datasets\u001b[0;34m(observation_details)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_observation_to_finetuning_datasets\u001b[39m(observation_details):\n\u001b[1;32m    117\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    called by validate_observation_for_finetuning() \u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m    calls add_observation_to_raw_finetuning_dataset() and add_observation_to_formatted_finetuning_dataset()\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     raw_finetuning_dataset_index, upload_timestamp \u001b[39m=\u001b[39m add_observation_to_raw_finetuning_dataset(observation_details)\n\u001b[1;32m    122\u001b[0m     add_observation_to_formatted_finetuning_dataset(observation_details, raw_finetuning_dataset_index, upload_timestamp)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:81\u001b[0m, in \u001b[0;36madd_observation_to_raw_finetuning_dataset\u001b[0;34m(observation_details)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m# append row to that file\u001b[39;00m\n\u001b[1;32m     80\u001b[0m upload_timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m---> 81\u001b[0m row \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mmeta.timestamp\u001b[39;49m\u001b[39m'\u001b[39;49m: upload_timestamp, \n\u001b[1;32m     82\u001b[0m                     \u001b[39m# 'meta.compatible_model_versions': [model_version],\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m                    \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mobservation_details[\u001b[39m'\u001b[39;49m\u001b[39mprompt_args\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     84\u001b[0m                    \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mobservation_details[\u001b[39m'\u001b[39;49m\u001b[39mcompletion_args\u001b[39;49m\u001b[39m'\u001b[39;49m]}, \n\u001b[1;32m     85\u001b[0m                    index\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     86\u001b[0m raw_finetuning_dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat((raw_finetuning_dataset, row), ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m raw_finetuning_dataset\u001b[39m.\u001b[39mto_csv(file_path, na_rep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNA\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/pandas/core/frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    656\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    657\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    663\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    664\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:123\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    122\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     arrays \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[1;32m    124\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:620\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    615\u001b[0m             val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[1;32m    617\u001b[0m         val \u001b[39m=\u001b[39m sanitize_array(\n\u001b[1;32m    618\u001b[0m             val, index, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, raise_cast_failure\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[0;32m--> 620\u001b[0m         com\u001b[39m.\u001b[39;49mrequire_length_match(val, index)\n\u001b[1;32m    622\u001b[0m     homogenized\u001b[39m.\u001b[39mappend(val)\n\u001b[1;32m    624\u001b[0m \u001b[39mreturn\u001b[39;00m homogenized\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interview_chatbot_venv/lib/python3.11/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (1)"
     ]
    }
   ],
   "source": [
    "# interview_graph.nodes[\"algorithm selection\"]['graph'].nodes['ask_how_it_works']['function_args']['subject'] = \"DBSCAN\"\n",
    "\n",
    "current_node = interview_graph.nodes[\"algorithm selection\"]['graph'].nodes[\"validate_why_applicant_picked_X_over_Y\"]\n",
    "\n",
    "current_node_function_args = deepcopy(current_node['function_args']) if 'function_args' in current_node else {}\n",
    "current_node_function_args['carryover_data'] = carryover_data\n",
    "current_node_function_args['current_section_name'] = \"algorithm selection\"\n",
    "current_node_function_args['validate_async'] = False\n",
    "current_node_function_args['chat_history_by_section'] = chat_history_by_section\n",
    "\n",
    "current_node_output = current_node['function'](**current_node_function_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3672603-1bbf-485e-9325-bc6e81ac826e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4a70ca-1852-4bf4-8df9-686ff3213576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.4 (20221203.1631)\n -->\n<!-- Pages: 1 -->\n<svg width=\"248pt\" height=\"332pt\"\n viewBox=\"0.00 0.00 248.48 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 244.48,-328 244.48,4 -4,4\"/>\n<!-- introduction -->\n<g id=\"node1\" class=\"node\">\n<title>introduction</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"120.24\" cy=\"-306\" rx=\"54\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"120.24\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">introduction</text>\n</g>\n<!-- algorithm selection -->\n<g id=\"node2\" class=\"node\">\n<title>algorithm selection</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"120.24\" cy=\"-234\" rx=\"79.5\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"120.24\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">algorithm selection</text>\n</g>\n<!-- introduction&#45;&gt;algorithm selection -->\n<g id=\"edge1\" class=\"edge\">\n<title>introduction&#45;&gt;algorithm selection</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.24,-287.7C120.24,-280.41 120.24,-271.73 120.24,-263.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123.74,-263.61 120.24,-253.61 116.74,-263.61 123.74,-263.61\"/>\n</g>\n<!-- dealing with categorical values -->\n<g id=\"node3\" class=\"node\">\n<title>dealing with categorical values</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"120.24\" cy=\"-162\" rx=\"120.48\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"120.24\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">dealing with categorical values</text>\n</g>\n<!-- algorithm selection&#45;&gt;dealing with categorical values -->\n<g id=\"edge2\" class=\"edge\">\n<title>algorithm selection&#45;&gt;dealing with categorical values</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.24,-215.7C120.24,-208.41 120.24,-199.73 120.24,-191.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123.74,-191.61 120.24,-181.61 116.74,-191.61 123.74,-191.61\"/>\n</g>\n<!-- dealing with numerical values -->\n<g id=\"node4\" class=\"node\">\n<title>dealing with numerical values</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"120.24\" cy=\"-90\" rx=\"117.78\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"120.24\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">dealing with numerical values</text>\n</g>\n<!-- dealing with categorical values&#45;&gt;dealing with numerical values -->\n<g id=\"edge3\" class=\"edge\">\n<title>dealing with categorical values&#45;&gt;dealing with numerical values</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.24,-143.7C120.24,-136.41 120.24,-127.73 120.24,-119.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123.74,-119.61 120.24,-109.61 116.74,-119.61 123.74,-119.61\"/>\n</g>\n<!-- conclusion -->\n<g id=\"node5\" class=\"node\">\n<title>conclusion</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"120.24\" cy=\"-18\" rx=\"48.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"120.24\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">conclusion</text>\n</g>\n<!-- dealing with numerical values&#45;&gt;conclusion -->\n<g id=\"edge4\" class=\"edge\">\n<title>dealing with numerical values&#45;&gt;conclusion</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.24,-71.7C120.24,-64.41 120.24,-55.73 120.24,-47.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123.74,-47.62 120.24,-37.62 116.74,-47.62 123.74,-47.62\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<AGraph <Swig Object of type 'Agraph_t *' at 0x144ba5f20>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_graph = graph.create_interview_flowchart()\n",
    "\n",
    "graph.plot_graph(interview_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12a3b3e-8c44-42d3-bdc4-d158d9eaea60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.4 (20221203.1631)\n -->\n<!-- Pages: 1 -->\n<svg width=\"973pt\" height=\"548pt\"\n viewBox=\"0.00 0.00 973.48 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-544 969.48,-544 969.48,4 -4,4\"/>\n<!-- confirm_what_applicant_did -->\n<g id=\"node1\" class=\"node\">\n<title>confirm_what_applicant_did</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"338.74\" cy=\"-522\" rx=\"112.5\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"338.74\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">confirm_what_applicant_did</text>\n</g>\n<!-- get_applicant_response -->\n<g id=\"node2\" class=\"node\">\n<title>get_applicant_response</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"338.74\" cy=\"-450\" rx=\"93\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"338.74\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_applicant_response</text>\n</g>\n<!-- confirm_what_applicant_did&#45;&gt;get_applicant_response -->\n<g id=\"edge1\" class=\"edge\">\n<title>confirm_what_applicant_did&#45;&gt;get_applicant_response</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.74,-503.7C338.74,-496.41 338.74,-487.73 338.74,-479.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"342.24,-479.61 338.74,-469.61 335.24,-479.61 342.24,-479.61\"/>\n</g>\n<!-- route_answer_to_ask_what_applicant_did -->\n<g id=\"node3\" class=\"node\">\n<title>route_answer_to_ask_what_applicant_did</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"338.74\" cy=\"-378\" rx=\"159.47\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"338.74\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">route_answer_to_ask_what_applicant_did</text>\n</g>\n<!-- get_applicant_response&#45;&gt;route_answer_to_ask_what_applicant_did -->\n<g id=\"edge2\" class=\"edge\">\n<title>get_applicant_response&#45;&gt;route_answer_to_ask_what_applicant_did</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.74,-431.7C338.74,-424.41 338.74,-415.73 338.74,-407.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"342.24,-407.61 338.74,-397.61 335.24,-407.61 342.24,-407.61\"/>\n</g>\n<!-- ask_what_applicant_did -->\n<g id=\"node4\" class=\"node\">\n<title>ask_what_applicant_did</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"181.74\" cy=\"-306\" rx=\"96.38\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"181.74\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">ask_what_applicant_did</text>\n</g>\n<!-- route_answer_to_ask_what_applicant_did&#45;&gt;ask_what_applicant_did -->\n<g id=\"edge3\" class=\"edge\">\n<title>route_answer_to_ask_what_applicant_did&#45;&gt;ask_what_applicant_did</title>\n<path fill=\"none\" stroke=\"black\" d=\"M300.74,-360.05C278.86,-350.3 251.2,-337.97 228.06,-327.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.72,-324.56 219.16,-323.69 226.87,-330.95 229.72,-324.56\"/>\n</g>\n<!-- validate_answer_how_it_works -->\n<g id=\"node5\" class=\"node\">\n<title>validate_answer_how_it_works</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"518.74\" cy=\"-306\" rx=\"122.38\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"518.74\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">validate_answer_how_it_works</text>\n</g>\n<!-- route_answer_to_ask_what_applicant_did&#45;&gt;validate_answer_how_it_works -->\n<g id=\"edge4\" class=\"edge\">\n<title>route_answer_to_ask_what_applicant_did&#45;&gt;validate_answer_how_it_works</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.86,-360.23C407.29,-350.34 439.65,-337.76 466.53,-327.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"467.45,-330.71 475.5,-323.82 464.91,-324.18 467.45,-330.71\"/>\n</g>\n<!-- route_answer_to_what_other_options_applicant_considered -->\n<g id=\"node6\" class=\"node\">\n<title>route_answer_to_what_other_options_applicant_considered</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"441.74\" cy=\"-234\" rx=\"222.5\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"441.74\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">route_answer_to_what_other_options_applicant_considered</text>\n</g>\n<!-- route_answer_to_ask_what_applicant_did&#45;&gt;route_answer_to_what_other_options_applicant_considered -->\n<g id=\"edge5\" class=\"edge\">\n<title>route_answer_to_ask_what_applicant_did&#45;&gt;route_answer_to_what_other_options_applicant_considered</title>\n<path fill=\"none\" stroke=\"black\" d=\"M346.4,-359.71C355.07,-341.09 370.26,-311.05 387.74,-288 395.2,-278.17 404.46,-268.41 413.19,-260\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"415.31,-262.81 420.23,-253.42 410.53,-257.7 415.31,-262.81\"/>\n</g>\n<!-- get_applicant_response&#45;1 -->\n<g id=\"node7\" class=\"node\">\n<title>get_applicant_response&#45;1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"100.74\" cy=\"-234\" rx=\"100.98\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"100.74\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_applicant_response&#45;1</text>\n</g>\n<!-- ask_what_applicant_did&#45;&gt;get_applicant_response&#45;1 -->\n<g id=\"edge6\" class=\"edge\">\n<title>ask_what_applicant_did&#45;&gt;get_applicant_response&#45;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M162.13,-288.05C152.16,-279.43 139.85,-268.8 128.89,-259.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.45,-256.91 121.6,-253.02 126.88,-262.2 131.45,-256.91\"/>\n</g>\n<!-- validate_answer_how_it_works&#45;&gt;route_answer_to_what_other_options_applicant_considered -->\n<g id=\"edge7\" class=\"edge\">\n<title>validate_answer_how_it_works&#45;&gt;route_answer_to_what_other_options_applicant_considered</title>\n<path fill=\"none\" stroke=\"black\" d=\"M500.1,-288.05C490.78,-279.58 479.33,-269.17 469.04,-259.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"471.62,-257.43 461.86,-253.3 466.91,-262.61 471.62,-257.43\"/>\n</g>\n<!-- ask_how_it_works -->\n<g id=\"node8\" class=\"node\">\n<title>ask_how_it_works</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"759.74\" cy=\"-234\" rx=\"77.5\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"759.74\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">ask_how_it_works</text>\n</g>\n<!-- validate_answer_how_it_works&#45;&gt;ask_how_it_works -->\n<g id=\"edge8\" class=\"edge\">\n<title>validate_answer_how_it_works&#45;&gt;ask_how_it_works</title>\n<path fill=\"none\" stroke=\"black\" d=\"M572.53,-289.38C610.87,-278.24 662.36,-263.28 701.64,-251.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"702.56,-255.25 711.19,-249.1 700.61,-248.53 702.56,-255.25\"/>\n</g>\n<!-- ask_what_other_options_applicant_considered -->\n<g id=\"node9\" class=\"node\">\n<title>ask_what_other_options_applicant_considered</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"186.74\" cy=\"-162\" rx=\"175.5\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"186.74\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">ask_what_other_options_applicant_considered</text>\n</g>\n<!-- route_answer_to_what_other_options_applicant_considered&#45;&gt;ask_what_other_options_applicant_considered -->\n<g id=\"edge9\" class=\"edge\">\n<title>route_answer_to_what_other_options_applicant_considered&#45;&gt;ask_what_other_options_applicant_considered</title>\n<path fill=\"none\" stroke=\"black\" d=\"M380.66,-216.23C343.49,-206.03 295.87,-192.96 257.12,-182.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"258.47,-179.06 247.9,-179.79 256.62,-185.81 258.47,-179.06\"/>\n</g>\n<!-- validate_why_applicant_picked_X_over_Y -->\n<g id=\"node10\" class=\"node\">\n<title>validate_why_applicant_picked_X_over_Y</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"581.74\" cy=\"-162\" rx=\"164\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"581.74\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">validate_why_applicant_picked_X_over_Y</text>\n</g>\n<!-- route_answer_to_what_other_options_applicant_considered&#45;&gt;validate_why_applicant_picked_X_over_Y -->\n<g id=\"edge10\" class=\"edge\">\n<title>route_answer_to_what_other_options_applicant_considered&#45;&gt;validate_why_applicant_picked_X_over_Y</title>\n<path fill=\"none\" stroke=\"black\" d=\"M475.99,-215.88C494.6,-206.57 517.82,-194.96 537.76,-184.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"539.12,-188.23 546.5,-180.63 535.99,-181.96 539.12,-188.23\"/>\n</g>\n<!-- get_applicant_response&#45;1&#45;&gt;route_answer_to_ask_what_applicant_did -->\n<g id=\"edge11\" class=\"edge\">\n<title>get_applicant_response&#45;1&#45;&gt;route_answer_to_ask_what_applicant_did</title>\n<path fill=\"none\" stroke=\"black\" d=\"M165.9,-248.15C211.57,-258.37 267.71,-273.23 286.74,-288 306.81,-303.57 320.61,-329.23 328.92,-349.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.61,-350.15 332.54,-358.16 332.12,-347.58 325.61,-350.15\"/>\n</g>\n<!-- get_applicant_response&#45;2 -->\n<g id=\"node11\" class=\"node\">\n<title>get_applicant_response&#45;2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"864.74\" cy=\"-162\" rx=\"100.98\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"864.74\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_applicant_response&#45;2</text>\n</g>\n<!-- ask_how_it_works&#45;&gt;get_applicant_response&#45;2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>ask_how_it_works&#45;&gt;get_applicant_response&#45;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M784.36,-216.59C797.98,-207.51 815.14,-196.07 830.09,-186.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"831.91,-189.1 838.29,-180.64 828.03,-183.27 831.91,-189.1\"/>\n</g>\n<!-- get_applicant_response&#45;3 -->\n<g id=\"node12\" class=\"node\">\n<title>get_applicant_response&#45;3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"186.74\" cy=\"-90\" rx=\"100.98\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"186.74\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_applicant_response&#45;3</text>\n</g>\n<!-- ask_what_other_options_applicant_considered&#45;&gt;get_applicant_response&#45;3 -->\n<g id=\"edge13\" class=\"edge\">\n<title>ask_what_other_options_applicant_considered&#45;&gt;get_applicant_response&#45;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M186.74,-143.7C186.74,-136.41 186.74,-127.73 186.74,-119.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"190.24,-119.61 186.74,-109.61 183.24,-119.61 190.24,-119.61\"/>\n</g>\n<!-- ask_why_applicant_picked_X_over_Y -->\n<g id=\"node13\" class=\"node\">\n<title>ask_why_applicant_picked_X_over_Y</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"452.74\" cy=\"-90\" rx=\"147.57\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"452.74\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">ask_why_applicant_picked_X_over_Y</text>\n</g>\n<!-- validate_why_applicant_picked_X_over_Y&#45;&gt;ask_why_applicant_picked_X_over_Y -->\n<g id=\"edge14\" class=\"edge\">\n<title>validate_why_applicant_picked_X_over_Y&#45;&gt;ask_why_applicant_picked_X_over_Y</title>\n<path fill=\"none\" stroke=\"black\" d=\"M550.18,-143.88C533.27,-134.7 512.22,-123.28 494.03,-113.4\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"495.76,-110.36 485.3,-108.67 492.43,-116.51 495.76,-110.36\"/>\n</g>\n<!-- empty_node -->\n<g id=\"node14\" class=\"node\">\n<title>empty_node</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"672.74\" cy=\"-90\" rx=\"54.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"672.74\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">empty_node</text>\n</g>\n<!-- validate_why_applicant_picked_X_over_Y&#45;&gt;empty_node -->\n<g id=\"edge15\" class=\"edge\">\n<title>validate_why_applicant_picked_X_over_Y&#45;&gt;empty_node</title>\n<path fill=\"none\" stroke=\"black\" d=\"M604.24,-143.7C616.01,-134.64 630.57,-123.44 643.24,-113.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"645.07,-116.7 650.86,-107.83 640.8,-111.15 645.07,-116.7\"/>\n</g>\n<!-- get_applicant_response&#45;2&#45;&gt;validate_answer_how_it_works -->\n<g id=\"edge16\" class=\"edge\">\n<title>get_applicant_response&#45;2&#45;&gt;validate_answer_how_it_works</title>\n<path fill=\"none\" stroke=\"black\" d=\"M866.35,-180.47C867.17,-200.7 865.08,-233.59 845.74,-252 817.09,-279.26 724.85,-292.53 645.61,-298.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"645.73,-295.45 636.03,-299.71 646.27,-302.43 645.73,-295.45\"/>\n</g>\n<!-- get_applicant_response&#45;3&#45;&gt;route_answer_to_what_other_options_applicant_considered -->\n<g id=\"edge17\" class=\"edge\">\n<title>get_applicant_response&#45;3&#45;&gt;route_answer_to_what_other_options_applicant_considered</title>\n<path fill=\"none\" stroke=\"black\" d=\"M263.32,-102.1C298.24,-109.8 338.81,-122.67 370.74,-144 394.39,-159.8 413.89,-185.96 426.43,-205.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"423.38,-207.55 431.56,-214.29 429.36,-203.92 423.38,-207.55\"/>\n</g>\n<!-- get_applicant_response&#45;4 -->\n<g id=\"node15\" class=\"node\">\n<title>get_applicant_response&#45;4</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"603.74\" cy=\"-18\" rx=\"100.98\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"603.74\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_applicant_response&#45;4</text>\n</g>\n<!-- ask_why_applicant_picked_X_over_Y&#45;&gt;get_applicant_response&#45;4 -->\n<g id=\"edge18\" class=\"edge\">\n<title>ask_why_applicant_picked_X_over_Y&#45;&gt;get_applicant_response&#45;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M488.91,-72.23C509.77,-62.56 536.2,-50.31 558.44,-40\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"559.68,-43.29 567.28,-35.9 556.73,-36.94 559.68,-43.29\"/>\n</g>\n<!-- get_applicant_response&#45;4&#45;&gt;validate_why_applicant_picked_X_over_Y -->\n<g id=\"edge19\" class=\"edge\">\n<title>get_applicant_response&#45;4&#45;&gt;validate_why_applicant_picked_X_over_Y</title>\n<path fill=\"none\" stroke=\"black\" d=\"M675.48,-30.91C698.98,-38.59 722.65,-51.22 736.74,-72 745.72,-85.24 746.05,-94.98 736.74,-108 725.28,-124.03 708.67,-135.26 690.54,-143.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"689.45,-139.8 681.43,-146.72 692.02,-146.31 689.45,-139.8\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<AGraph <Swig Object of type 'Agraph_t *' at 0x144ba6880>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.plot_graph(interview_graph.nodes[\"algorithm selection\"]['graph'])\n",
    "\n",
    "graph.plot_graph(interview_graph.nodes[\"dealing with categorical values\"]['graph'])\n",
    "\n",
    "graph.plot_graph(interview_graph.nodes[\"dealing with numerical values\"]['graph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b1541-bbe0-423d-b713-6405f42aac19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4aa9be3-b2dc-4cb1-bfe1-b3785920055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "applicant_approaches = {\n",
    "    'algorithm selection': 'GAN',\n",
    "    'categorical encoding': 'hash encoding',\n",
    "    'handling numerical variables': 'robust scaler'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_challenge_objectives_and_contraints = \"\"\"- Maximize model performance.\n",
    "- Model interpretability is not a strong requirement.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "885c55e0-735b-42ba-b927-41e9004086e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_interviewer_thinks_applicant_has_done_in_ipynb = deepcopy(applicant_approaches)\n",
    "# update what_interviewer_thinks_applicant_has_done_in_ipynb here\n",
    "\n",
    "graph.add_function_arg_to_node(interview_graph.nodes[\"algorithm selection\"]['graph'].nodes['confirm_what_applicant_did'],\n",
    "                        'context',\n",
    "                         what_interviewer_thinks_applicant_has_done_in_ipynb\n",
    "                        )\n",
    "graph.add_function_arg_to_node(interview_graph.nodes[\"dealing with categorical values\"]['graph'].nodes['confirm_what_applicant_did'],\n",
    "                        'context',\n",
    "                         what_interviewer_thinks_applicant_has_done_in_ipynb\n",
    "                        )\n",
    "graph.add_function_arg_to_node(interview_graph.nodes[\"dealing with numerical values\"]['graph'].nodes['confirm_what_applicant_did'],\n",
    "                        'context',\n",
    "                         what_interviewer_thinks_applicant_has_done_in_ipynb\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d437360-77f9-40cb-8592-a954654c354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "carryover_data = {\n",
    "    \"applicant_data\": {\"applicant_skill_summary\": {\"technical_depth\": np.random.randint(1, 4),\n",
    "                                                \"technical_breadth\": np.random.randint(1, 4),\n",
    "                                                \"critical_thinking\": np.random.randint(1, 4)},\n",
    "                       \"applicant_context\": applicant_approaches,\n",
    "                      },\n",
    "    \"what_interviewer_thinks_applicant_has_done_in_ipynb\": what_interviewer_thinks_applicant_has_done_in_ipynb,\n",
    "    \"data_challenge_objectives_and_contraints\": data_challenge_objectives_and_contraints,\n",
    "}\n",
    "\n",
    "# each list within this variable is the conversation in a particular section. each string within that list is the ordered dialogue from each member of the interview\n",
    "chat_history_by_section = []\n",
    "flattened_chat_history = \"\"\n",
    "\n",
    "# list of the nodes that have been visited till the current point\n",
    "sections_visited_names = []\n",
    "nodes_visited_names = []\n",
    "\n",
    "validate_async = False\n",
    "is_interview_complete = False\n",
    "\n",
    "\n",
    "current_section_name, current_section = graph.get_first_node_in_graph(interview_graph)\n",
    "current_section_graph = current_section['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df3d3ba-d391-43d0-8e7a-9e9cef72e600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************\n",
      "********* Model Name *********\n",
      "route_answer_to_what_other_options_applicant_considered\n",
      "********* Prompt *********\n",
      "Current section chat:\n",
      "Interviewer: Could you confirm whether you used a Generative Adversarial Network for this task?\n",
      "Applicant: Yes, I used a Generative Adversarial Network for this task.\n",
      "Interviewer: How does a Generative Adversarial Network work?\n",
      "Applicant: GANs are deep learning models that produce fake data.\n",
      "Interviewer: Besides GANs, did you consider any other algorithms for this task?\n",
      "Applicant: Yes, I considered using autoencoders and also CNNs.\n",
      "\n",
      "Details:\n",
      "Is routing correct: 1\n",
      "###\n",
      "\n",
      "********* Completion *********\n",
      "Route: 1\n",
      "******************\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m current_node_function_args[\u001b[39m'\u001b[39m\u001b[39mchat_history_by_section\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m chat_history_by_section\n\u001b[1;32m     18\u001b[0m \u001b[39m# call function\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m current_node_output \u001b[39m=\u001b[39m current_node[\u001b[39m'\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m'\u001b[39;49m](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcurrent_node_function_args)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m current_node_output[\u001b[39m'\u001b[39m\u001b[39mnew_chat_lines\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     22\u001b[0m     \u001b[39m# store return values in appropriate variables (chat_history)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     chat_history_by_section[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mextend(current_node_output[\u001b[39m'\u001b[39m\u001b[39mnew_chat_lines\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/nodes/node_functions.py:294\u001b[0m, in \u001b[0;36mroute_answer_to_what_other_options_applicant_considered\u001b[0;34m(carryover_data, validate_async, chat_history_by_section, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m completion_args \u001b[39m=\u001b[39m parse(model_metadata[\u001b[39m'\u001b[39m\u001b[39mcompletion_template\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    285\u001b[0m                         completion)\u001b[39m.\u001b[39mnamed\n\u001b[1;32m    286\u001b[0m observation_details \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(model_name\u001b[39m=\u001b[39mmodel_name, \n\u001b[1;32m    287\u001b[0m                            model_version\u001b[39m=\u001b[39mmodel_version,\n\u001b[1;32m    288\u001b[0m                            prompt_template\u001b[39m=\u001b[39mmodel_metadata[\u001b[39m'\u001b[39m\u001b[39mprompt_template\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m                            prompt \u001b[39m=\u001b[39m observation_prompt,\n\u001b[1;32m    293\u001b[0m                            completion\u001b[39m=\u001b[39mcompletion)\n\u001b[0;32m--> 294\u001b[0m completion_args_to_use_for_interview \u001b[39m=\u001b[39m submit_observation_for_finetuning_validation(observation_details, \n\u001b[1;32m    295\u001b[0m                                                                                     validate_async\u001b[39m=\u001b[39;49mvalidate_async)\n\u001b[1;32m    297\u001b[0m routing_value \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(completion_args_to_use_for_interview[\u001b[39m'\u001b[39m\u001b[39mroute\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(routing_value\u001b[39m=\u001b[39mrouting_value, \n\u001b[1;32m    299\u001b[0m             new_chat_lines\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:195\u001b[0m, in \u001b[0;36msubmit_observation_for_finetuning_validation\u001b[0;34m(observation_details, validate_async)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m# TODO: save queue to file before validating live and then load again, in case an error occurs and kills the program.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m validate_async:\n\u001b[0;32m--> 195\u001b[0m     result \u001b[39m=\u001b[39m validate_observation_for_finetuning(observation_details)\n\u001b[1;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mvalidated\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    197\u001b[0m         finetuning_validation_queue\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/Documents/ds_interview_chatbot/ds_interviewer/finetuning/prepare_data.py:145\u001b[0m, in \u001b[0;36mvalidate_observation_for_finetuning\u001b[0;34m(observation_details)\u001b[0m\n\u001b[1;32m    141\u001b[0m model_metadata, model_version \u001b[39m=\u001b[39m get_model_metadata(observation_details[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m], observation_details[\u001b[39m\"\u001b[39m\u001b[39mmodel_version\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_version\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m observation_details\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[39mprint\u001b[39m(print_template\u001b[39m.\u001b[39mformat(observation_details[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    143\u001b[0m                             observation_details[\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    144\u001b[0m                             observation_details[\u001b[39m'\u001b[39m\u001b[39mcompletion\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m--> 145\u001b[0m validate_now \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mDo you want to validate this observation now? (1/0): \u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m validate_now:\n\u001b[1;32m    148\u001b[0m     label \u001b[39m=\u001b[39m get_label_for_correct_or_incorrect_completion()\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# traverse through sections\n",
    "while not is_interview_complete:\n",
    "    chat_history_by_section.append([])\n",
    "    current_node_name, current_node = graph.get_first_node_in_graph(current_section_graph)\n",
    "    # print(\"\\n\\ncurrent section: \" + current_section_name)\n",
    "    # print(\"current node: \" + current_node_name)\n",
    "    is_section_complete = False\n",
    "    # traverse within section\n",
    "    while not is_section_complete:\n",
    "        # access current node from current section\n",
    "        # prep args for current node function\n",
    "        current_node_function_args = deepcopy(current_node['function_args']) if 'function_args' in current_node else {}\n",
    "        current_node_function_args['current_section_name'] = current_section_name\n",
    "        current_node_function_args['carryover_data'] = carryover_data\n",
    "        current_node_function_args['validate_async'] = validate_async\n",
    "        current_node_function_args['chat_history_by_section'] = chat_history_by_section\n",
    "\n",
    "        # call function\n",
    "        current_node_output = current_node['function'](**current_node_function_args)\n",
    "        \n",
    "        if current_node_output['new_chat_lines']:\n",
    "            # store return values in appropriate variables (chat_history)\n",
    "            chat_history_by_section[-1].extend(current_node_output['new_chat_lines'])\n",
    "            flattened_chat_history += \"\\n\" + \"\\n\".join(current_node_output['new_chat_lines'])\n",
    "            clear_output()\n",
    "            print(flattened_chat_history)\n",
    "    \n",
    "        # update nodes_visited\n",
    "        nodes_visited_names.append(current_node_name)\n",
    "    \n",
    "        # figure out next node to visit\n",
    "        current_node_name, current_node = graph.get_next_node(current_section_graph, current_node_name, current_node_output)\n",
    "        if current_node_name is None:\n",
    "            # prepare routing_value (result of a section)\n",
    "            current_section_output = dict(routing_value=1, \n",
    "                                          new_chat_line=None) \n",
    "            is_section_complete = True\n",
    "\n",
    "    \n",
    "    sections_visited_names.append(current_section_name)\n",
    "    # get new section\n",
    "    current_section_name, current_section = graph.get_next_node(interview_graph, current_section_name, current_section_output)\n",
    "    if current_section_name is None:\n",
    "        is_interview_complete = True\n",
    "        break\n",
    "    current_section_graph = current_section['graph']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20174281-a903-4871-8d66-90c339f1d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data.validate_observations_for_finetuning_from_queue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_interview_chatbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "41f12de9af4222e4b257cc56cd09168715a90beb6fcd186c29ab659bfe87472a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
