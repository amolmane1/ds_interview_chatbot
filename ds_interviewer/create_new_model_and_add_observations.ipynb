{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca76c9f-4da6-4d06-a90b-0d1d9f153bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.models_metadata import get_model_metadata, add_new_model_version\n",
    "from finetuning import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f532bb2-caa5-4b3c-963b-39b22b0ae9d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ask_what_you_did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a692111f-0299-4e1f-928d-f12054f0d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'prompt_template': \\\n",
    "\"\"\"Details:\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Subject: {subject}\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Interviewer: {interviewer_dialogue}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Subject: $<the subject Interviewer is to ask a question about>\n",
    "Is question correct: $<1 or 0 - whether the question Interviewer asks is of the right type or not>\n",
    "###\n",
    "Interviewer: $<Interviewer's question about the subject>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"ask_what_you_did\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e108e757-17b0-40ec-b80f-46514ebf2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"ask_what_you_did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ba0579-3f5c-458d-806c-2264d543ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"NA\",\n",
    "    subject=\"Dealing with class imbalance\",\n",
    "                  is_completion_correct=1)\n",
    "completion_args = dict(interviewer_dialogue=\"How did you deal with class imbalance in the dataset?\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"ask_what_you_did\", \n",
    "                               model_version=\"09.01.23-1\",\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a63a8-1f2b-4266-b08e-e3e221607eff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# confirm_what_applicant_did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157d7fe4-010e-4ec0-b016-d3f74bb614a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Section: {section}\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Interviewer: {interviewer_dialogue}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question in the following format:\n",
    "\n",
    "Context:\n",
    "$<a summary of the techniques the applicant has used for each section of the interview>\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Section: $<the section Interviewer is to ask a question about>\n",
    "Is question correct: $<1 or 0 - whether the question Interviewer asks is of the right type or not>\n",
    "###\n",
    "Interviewer: $<Interviewer's question about the subject>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"confirm_what_applicant_did\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8c0f79-1f47-4a2d-9325-11a4e75b9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"confirm_what_applicant_did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425dc24a-a7a3-446b-853a-be7175417e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    context=\"\"\"{\"algorithm selection\": \"SVM\"}\"\"\",\n",
    "    current_section_chat=\"NA\",\n",
    "    section=\"algorithm selection\",\n",
    "                  is_completion_correct=1)\n",
    "completion_args = dict(interviewer_dialogue=\"From your submission, I saw that you used SVM to learn the data. Could you confirm this?\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"confirm_what_applicant_did\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252adbaa-0ee2-4e26-bc1a-1d79f27c1f2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# route_answer_to_confirm_what_applicant_did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408d6487-019c-4d9c-a06b-49e5629f062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is routing correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Route: {route}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer assigns Applicant's answer to integer categories in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is routing correct: $<1 or 0 - whether the category Interviewer assigns to Applicant's answer is of the right type or not>\n",
    "###\n",
    "Route: $<whether applicant confirms that they used a certain approach (1) or not (0)>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"route_answer_to_confirm_what_applicant_did\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2344f6-ea73-4403-a6c0-78d15793abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"route_answer_to_confirm_what_applicant_did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62296b61-0a9f-433b-bb01-95522033a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: Could you confirm whether you used SVM for training on the data?\\nApplicant: Yeah, I did.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"route_answer_to_confirm_what_applicant_did\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045d2926-2e3d-4ed6-b833-838d75cacf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: Could you confirm whether you used one hot encoding to encode the vategorical variables?\\nApplicant: No, I didn't use that.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"0\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"route_answer_to_confirm_what_applicant_did\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a2b43-9b96-49f4-baec-e24a1b33cef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ask_what_other_options_applicant_considered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e852b-880f-4fe2-b7a2-ba95a3266e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019f970d-31b4-4dba-948b-cb1dd11e1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Interviewer: {interviewer_dialogue}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is question correct: $<1 or 0 - whether the question Interviewer asks is correct or not>\n",
    "###\n",
    "Interviewer: $<Interviewer's question about which other options the applicant considered for a particular topic>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"ask_what_other_options_applicant_considered\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5857c01-bc3d-4b81-adfe-2d1f9557f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"ask_what_other_options_applicant_considered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cac78d-4290-4f31-a8f3-b5edcbcd0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: From your submission, I see that you used a Random Forest model. Is that correct?\\nApplicant: No.\\nInterviewer: What algorithm did you use, then?\\nApplicant: I played around with Random Forest, but I ended up using SVM.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(interviewer_dialogue=\"Besides Random Forest and SVM, did you consider any other options?\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"ask_what_other_options_applicant_considered\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2cc15-12f8-4623-971a-577bb524fa5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66fefd2-e5af-4115-832e-f0156ac09827",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Interviewer: {interviewer_dialogue}\n",
    "$$$\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is question correct: $<1 or 0 - whether the question Interviewer asks is correct or not>\n",
    "###\n",
    "Interviewer: $<Interviewer's question about which other options the applicant considered for a particular topic>\n",
    "$$$\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"ask_what_other_options_applicant_considered\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686f7a8-45ff-4be9-b4ed-2bfef878166c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# route_answer_to_ask_what_applicant_did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e212b9af-7f5b-4459-aaba-a57ebf34fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is routing correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Route: {route}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer assigns Applicant's answer to integer categories in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is routing correct: $<1 or 0 - whether the category Interviewer assigns to Applicant's answer is of the right type or not>\n",
    "###\n",
    "Route: $<whether applicant used an approach for a particular task (1) or did nothing (0)>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(\"route_answer_to_ask_what_applicant_did\", new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5feff5-1099-459f-a072-3a4f9f69077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(\"route_answer_to_ask_what_applicant_did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419196b4-6d29-4887-9f12-38918e46b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: What algorithm did you use for training on the data?\\nApplicant: I used Lasso.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"route_answer_to_ask_what_applicant_did\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59461f15-af9a-4324-9909-950f7b6c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: How did you tune the hyperparameters in your model?\\nApplicant: I didn't tune my hypers.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"0\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=\"route_answer_to_ask_what_applicant_did\", \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4188a7-c504-4d31-a74f-e3889d50339a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# validate_answer_how_it_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2784737-6123-49bd-9d0d-efe644ea2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"validate_answer_how_it_works\"\n",
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58aeac8-e20e-4373-86fd-ea3db681a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current section chat:\n",
      "{current_section_chat}\n",
      "\n",
      "Details:\n",
      "Question type: {question_type}\n",
      "Is validation correct: {is_completion_correct}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_metadata['prompt_template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47920d6f-fb6a-4b3d-91f3-39b2aa744912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Response: {correct_response}\n",
      "Validation of response: {validation_of_response}\n"
     ]
    }
   ],
   "source": [
    "print(model_metadata['completion_template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a9c3a0-0c83-4f11-83b6-71b1749cc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: What algorithm did you use for training on the data?\\nApplicant: I used Lasso.\",\n",
    "    question_type=\"how it works\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(correct_response=\"NA\",\n",
    "                       validation_of_response=\"-1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c61b4a0-ad09-47c6-ac96-87433d4de721",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: From your submission, I see that you used Standard Scaler on the numerical variables. Is that correct?\\nApplicant: I tried it at first, but didn't use it in my final solution.\\nInterviewer: I see. How did you scale the numerical values then?\\nApplicant: I decided not to scale my values, because I was using a Decision Tree, which doesnt require the numerical values to be scaled.\",\n",
    "    question_type=\"how it works\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(correct_response=\"NA\",\n",
    "                       validation_of_response=\"-1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90fcd9-f31a-41e7-bf34-3641ee72d588",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# route_answer_to_what_other_options_applicant_considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc1b35b-f5c3-47af-a45c-4e7f176cfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"route_answer_to_what_other_options_applicant_considered\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fa000-d744-40ca-a978-7adba56d8084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81affd9-446d-4534-a079-3bd6f95eba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is routing correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Route: {route}\n",
    "$$$\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer assesses whether Applicant has considered other options to solve a particular task, and assigns Applicant's answer to integer categories in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is routing correct: $<1 or 0 - whether the category Interviewer assigns to Applicant's answer is of the right type or not>\n",
    "###\n",
    "Route: $<whether applicant has not yet mentioned what other options they considered (-1) / not considered any other options (0) / considered other options (1)>\n",
    "$$$\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90beb920-d8d8-4dd9-8773-154a2e37d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f020deed-c85d-4cb4-99cd-1229392ff757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_version': '17.01.23',\n",
       " 'prompt_template': 'Current section chat:\\n{current_section_chat}\\n\\nDetails:\\nIs routing correct: {is_completion_correct}\\n###\\n',\n",
       " 'completion_template': 'Route: {route}\\n$$$',\n",
       " 'kshot_header': \"Interviewer is interviewing Applicant for a job as a Data Scientist.\\nInterviewer assesses whether Applicant has considered other options to solve a particular task, and assigns Applicant's answer to integer categories in the following format:\\n\\nCurrent section chat:\\n$<Conversation so far between Interviewer and Applicant. This may be empty.>\\n\\nDetails:\\nIs routing correct: $<1 or 0 - whether the category Interviewer assigns to Applicant's answer is of the right type or not>\\n###\\nRoute: $<whether applicant has not yet mentioned what other options they considered (-1) / not considered any other options (0) / considered other options (1)>\\n$$$\\n\\nBelow are some correct examples:\\n\\n\",\n",
       " 'finetuned_model_name': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d09becc-5345-49ff-89d8-1420f12784a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: Besides Lasso, what other algorithms did you consider using for training on the data?\\nApplicant: I thought of SVM and Random Forest.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f064e7-6b76-4181-9b9b-6d08bf6df49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: Besides One Hotting, what other approaches did you consider using for handling categorical data?\\nApplicant: I didn't think of any others.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"0\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f43528d-89ce-4c62-b6d2-9498d1d48a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"Interviewer: How does Standard Scaler work?\\nApplicant: It transforms the data to have a mean of 0 and a standard deviation of 1.\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(route=\"-1\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4b212-26dd-4927-a585-ea212734ece1",
   "metadata": {},
   "source": [
    "### version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679d949a-de17-41a1-97a4-2011977eaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is routing correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Route: {route}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer assesses whether Applicant has considered other options to solve a particular task, and assigns Applicant's answer to integer categories in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is routing correct: $<1 or 0 - whether the category Interviewer assigns to Applicant's answer is of the right type or not>\n",
    "###\n",
    "Route: $<whether applicant has not yet mentioned what other options they considered (-1) / not considered any other options (0) / considered other options (1)>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09376a-a71a-451d-a8e6-737a4c8cd01d",
   "metadata": {},
   "source": [
    "# respond_to_interviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766dee3d-a9fc-4b06-81a9-03fd63b07b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"respond_to_interviewer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d171c2-6512-4789-9ed1-d7d5868c657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'model_version': None,\n",
    "    'prompt_template': \\\n",
    "\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Applicant skill summary: {applicant_skill_summary}\n",
    "Is response correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Applicant: {applicant_dialogue}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Applicant responds to Interviewer's questions in the following format:\n",
    "\n",
    "Context:\n",
    "$<a summary of the techniques the applicant has used for each section of the interview, the insights they have gained from exploring the data, etc>\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Applicant skill summary: $<>\n",
    "Is response correct: $<1 or 0 - whether the response Applicant gives is correct or not>\n",
    "###\n",
    "Applicant: $<Applicant's response to Interviewer's question>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None\n",
    "}\n",
    "\n",
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f8ebb2-1b73-4548-a851-2088911d3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f51a63-cfb1-4dda-80bf-a1afc6116a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_args = dict(\n",
    "#     context=\"\"\"{\"algorithm selection\": \"SVM\"}\"\"\",\n",
    "#     current_section_chat=\"NA\",\n",
    "#     section=\"algorithm selection\",\n",
    "#                   is_completion_correct=1)\n",
    "# completion_args = dict(interviewer_dialogue=\"From your submission, I saw that you used SVM to learn the data. Could you confirm this?\")\n",
    "# observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "# completion = model_metadata['completion_template'].format(**completion_args)\n",
    "# observation_details = dict(model_name=\"confirm_what_applicant_did\", \n",
    "#                                model_version=model_version,\n",
    "#                                prompt_template=model_metadata['prompt_template'], \n",
    "#                                completion_template=model_metadata['completion_template'], \n",
    "#                                prompt_args=prompt_args, \n",
    "#                                completion_args=completion_args,\n",
    "#                                prompt = observation_prompt,\n",
    "#                                completion=completion)\n",
    "\n",
    "# prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a23d3f74",
   "metadata": {},
   "source": [
    "# ask_why_applicant_picked_X_over_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c1863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ask_why_applicant_picked_X_over_Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a39eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'prompt_template': \\\n",
    "\"\"\"Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "    'completion_template': \\\n",
    "\"\"\"Subject 1: {subject_1}\n",
    "Subject 2: {subject_2}\n",
    "Interviewer: {interviewer_dialogue}\"\"\",\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question in the following format:\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is question correct: $<1 or 0 - whether the question Interviewer asks is correct or not>\n",
    "###\n",
    "Subject 1: $<approach applicant chose to use>\n",
    "Subject 2: $<approach applicant considered using, but decided not to. If applicant didn't consider any other approaches, this should be 'NA'>\n",
    "Interviewer: $<Interviewer's question about why they chose their approach instead of the other>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "    'finetuned_model_name': None,\n",
    "    'stop_sequence': \"$$$\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36377bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)\n",
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9989731",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    current_section_chat=\"\"\"Applicant: Yes, I used Binary Encoding on my categorical variables.\n",
    "Interviewer: How does Binary Encoding work?\n",
    "Applicant: Binary Encoding is a technique used to encode categorical variables by assigning each value a binary representation. This binary representation can then be used as a numerical representation of the original value. This approach is useful when dealing with large datasets, as it simplifies the data and reduces the dimensionality.\n",
    "Interviewer: Other than Binary Encoding, what other techniques did you consider for encoding the categorical values?\n",
    "Applicant: I didn't consider any other approaches.\"\"\",\n",
    "    is_completion_correct=1)\n",
    "completion_args = dict(approach_chosen=\"Binary Encoding\",\n",
    "    approaches_considered=\"NA\",\n",
    "    interviewer_dialogue=\"Why did you use Binary Encoding over One Hot Encoding?\")\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02b3727a",
   "metadata": {},
   "source": [
    "# validate_why_applicant_picked_X_over_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6425112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"validate_why_applicant_picked_X_over_Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a39eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'prompt_template': \\\n",
    "\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is validation correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "\n",
    "    'completion_template': \\\n",
    "\"\"\"Validation of response: {validation_of_response}\n",
    "Applicant justifications:\n",
    "{applicant_justifications}\n",
    "Number of correct, incorrect, irrelevant subpoints: {num_correct_subpoints}, {num_incorrect_subpoints}, {num_irrelevant_subpoints}\"\"\",\n",
    "\n",
    "\"nested_completion_templates\": dict(applicant_justifications = dict(template = \"<{subpoint}><{validation_of_subpoint}><{reason_for_validation}>\",\n",
    "                                                                    delimiter = \"Subpoint: \")\n",
    "                                    ),\n",
    "\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer validates Applicant's answer to a question in the following format:\n",
    "\n",
    "Context:\n",
    "$<a summary of the techniques the applicant has used for each section of the interview, the insights gained from any analysis, the objective of the case study, and any constraints/requirements to applicant's solution>\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant. This may be empty.>\n",
    "\n",
    "Details:\n",
    "Is validation correct: $<1 or 0 - whether Interviewer's validation of applicant's response is correct or not>\n",
    "###\n",
    "Validation of response: $<whether Applicant's response is -1=Applicant has not given answer yet / 0=Applicant Did Not Understand question / 1=applicant has given an answer>\n",
    "Applicant justifications:\n",
    "$<The subpoints applicant makes in their response to interviewer's question, in the following format (one line for each subpoint): \"Subpoint: <subpoint> <interviewer's assessment of whether that subpoint is correct (1), incorrect (-1), or irrelevant (0).> <Reason for why the subpoint is incorrect, if it is incorrect. If it is valid/irrelevant, output \"NA\">\". If applicant makes no subpoints, output \"NA\">\n",
    "Number of correct, incorrect, irrelevant subpoints: $<the number of correct subpoints applicant made>, $<the number of incorrect subpoints applicant made>, $<the number of irrelevant subpoints applicant made>\n",
    "\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    'finetuned_model_name': None,\n",
    "\n",
    "    'stop_sequence': \"$$$\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41124d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36377bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9989731",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    context=\"\"\"What applicant did:\n",
    "- algorithm selection: Decision Tree\n",
    "\n",
    "Objectives:\n",
    "- Mamimize model Recall.\n",
    "\"\"\",\n",
    "    current_section_chat=\"\"\"Interviewer: Why did you use Binary Encoding over One Hot Encoding on the categorical variables?\n",
    "Applicant: Because binary encoding would require fewer columns to encode the categorical columns than one hot encoding, which would enable my tree-based model to perform better.\"\"\",\n",
    "    is_completion_correct=1)\n",
    "    \n",
    "completion_args = dict(validation_of_response=\"1\",\n",
    "    applicant_justifications=\"\"\"Subpoint: <Binary Encoding requires fewer columns to encode a categorical variable than One Hot Encoding.><1><NA>\n",
    "Subpoint: <Tree-based models perform better with fewer columns.><1><NA>\n",
    "\"\"\",\n",
    "    num_correct_subpoints=\"2\", \n",
    "    num_incorrect_subpoints=\"0\", \n",
    "    num_irrelevant_subpoints=\"0\")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238f63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    context=\"\"\"What applicant did:\n",
    "- algorithm selection: Neural Net\n",
    "\n",
    "Objectives:\n",
    "- Minimize the number of false positives from the model.\n",
    "- Build a model that is sufficiently interpretable / explainable.\"\"\",\n",
    "    current_section_chat=\"\"\"Interviewer: Why did you use Recall instead of Precision as a scoring metric to optimize the model's performance?\n",
    "Applicant: Because the client wanted to catch as many positives as possible, and Recall measures what percentage of actual positives the model predicts as positive.\"\"\",\n",
    "    is_completion_correct=1)\n",
    "    \n",
    "completion_args = dict(validation_of_response=\"1\",\n",
    "    applicant_justifications=\"\"\"Subpoint: <Client wanted to catch as many positives as possible.><-1><The objective was to minimize the number of false positives from the model.>\n",
    "Subpoint: <Recall measures what percentage of actual positives the model predicts as positive.><1><NA>\n",
    "\"\"\",\n",
    "    num_correct_subpoints=\"1\", \n",
    "    num_incorrect_subpoints=\"1\", \n",
    "    num_irrelevant_subpoints=\"0\")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5319a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    context=\"\"\"What applicant did:\n",
    "- algorithm selection: Linear Regression\n",
    "\n",
    "Objectives:\n",
    "- Minimize the number of false positives from the model.\n",
    "\n",
    "Constraints:\n",
    "- Model should be be able to make predictions quickly.\n",
    "\"\"\",\n",
    "    current_section_chat=\"\"\"Interviewer: Why did you use a Linear Regression model instead of KNN?\n",
    "Applicant: KNNs take a longer time to make predictions, and Linear models are more interpretable.\"\"\",\n",
    "    is_completion_correct=1)\n",
    "    \n",
    "completion_args = dict(validation_of_response=\"1\",\n",
    "    applicant_justifications=\"\"\"Subpoint: <KNNs take a longer time to make predictions than Linear models.><-1><NA>\n",
    "Subpoint: <Linear models are more interpretable than KNN.><0><The model is not required to be interpretable.>\n",
    "\"\"\",\n",
    "    num_correct_subpoints=\"1\", \n",
    "    num_incorrect_subpoints=\"0\", \n",
    "    num_irrelevant_subpoints=\"1\")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56313f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict(\n",
    "    context=\"\"\"What applicant did:\n",
    "- algorithm selection: Linear Regression\n",
    "\n",
    "Objectives:\n",
    "- Minimize the number of false positives from the model.\n",
    "\n",
    "Constraints:\n",
    "- Model should be be able to make predictions quickly.\n",
    "\"\"\",\n",
    "    current_section_chat=\"\"\"Interviewer: Besides Linear Regression, what other algorithms did you consider?\n",
    "Applicant: I considered using a Decision Tree and a Neural Net.\"\"\",\n",
    "    is_completion_correct=1)\n",
    "    \n",
    "completion_args = dict(validation_of_response=\"-1\",\n",
    "    applicant_justifications=\"\"\"NA\n",
    "\"\"\",\n",
    "    num_correct_subpoints=\"0\", \n",
    "    num_incorrect_subpoints=\"0\", \n",
    "    num_irrelevant_subpoints=\"0\")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca3ca2e",
   "metadata": {},
   "source": [
    "# validate_answer_devils_advocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c921114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"validate_answer_devils_advocate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d353f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = {\n",
    "    'prompt_template': \\\n",
    "\"\"\"What applicant did for each section:\n",
    "{what_applicant_did_for_each_section}\n",
    "\n",
    "Objectives/Constraints:\n",
    "{objectives_and_constraints}\n",
    "\n",
    "Insights:\n",
    "{insights}\n",
    "\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is validation correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "\n",
    "    'completion_template': \\\n",
    "\"\"\"Validation of response: {validation_of_response}\n",
    "\n",
    "Applicant justifications:\n",
    "{applicant_justifications}\n",
    "\n",
    "Number of correct, incorrect, irrelevant subpoints: {num_correct_subpoints}, {num_incorrect_subpoints}, {num_irrelevant_subpoints}\n",
    "\"\"\",\n",
    "\n",
    "\"nested_completion_templates\": dict(applicant_justifications = dict(template = \"<{subpoint}><{validation_of_subpoint}><{reason_for_validation}>\",\n",
    "                                                                    delimiter = \"Subpoint: \")\n",
    "                                    ),\n",
    "\n",
    "    'kshot_header': \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer validates Applicant's answer to the question \"But the issue with your approach is <>. Do you still think it is the optimal choice?\" in the following format:\n",
    "\n",
    "What applicant did for each section:\n",
    "$<a summary of the techniques the applicant has used for each section of the interview>\n",
    "\n",
    "Objectives/Constraints:\n",
    "$<the objective of the case study, and any constraints/requirements to applicant's solution>\n",
    "\n",
    "Insights:\n",
    "$<any insights about the problem>\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant>\n",
    "\n",
    "Details:\n",
    "Is validation correct: $<1 or 0 - whether Interviewer's validation of applicant's response is correct or not>\n",
    "###\n",
    "Validation of response: $<whether Applicant's response is -1=Applicant has not given answer yet / 0=Applicant Did Not Understand question / 1=applicant has given an answer>\n",
    "\n",
    "Applicant justifications:\n",
    "$<The subpoints applicant makes in their response to interviewer's question, in the following format (one line for each subpoint): \"Subpoint: <subpoint> <interviewer's assessment of whether that subpoint is correct (1), incorrect (-1), or irrelevant (0).> <Reason for why the subpoint is incorrect, if it is incorrect. If it is valid/irrelevant, output \"NA\">\". If applicant makes no subpoints, output \"NA\">\n",
    "\n",
    "Number of correct, incorrect, irrelevant subpoints: $<the number of correct subpoints applicant made>, $<the number of incorrect subpoints applicant made>, $<the number of irrelevant subpoints applicant made>\n",
    "\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    'finetuned_model_name': None,\n",
    "\n",
    "    'stop_sequence': \"$$$\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9544c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa46a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f39b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict( \n",
    "\n",
    "what_applicant_did_for_each_section={\n",
    "    \"categorical encoding\" : 'binary encoding',\n",
    "},\n",
    "\n",
    "objectives_and_constraints = [\n",
    "    \"Model must be interpretable.\"\n",
    "],\n",
    "\n",
    "insights = [],\n",
    "\n",
    "current_section_chat=\"\"\"Interviewer: Why did you use Binary Encoding over One Hot Encoding on the categorical variables?\n",
    "Applicant: Because binary encoding would require fewer columns to encode the categorical columns than one hot encoding, which would enable my tree-based model to perform better.\"\"\",\n",
    "\n",
    "is_completion_correct=1\n",
    ")\n",
    "    \n",
    "completion_args = dict(\n",
    "validation_of_response=\"-1\",\n",
    "\n",
    "applicant_justifications=\"\"\"NA\"\"\",\n",
    "\n",
    "num_correct_subpoints=\"0\", \n",
    "num_incorrect_subpoints=\"0\", \n",
    "num_irrelevant_subpoints=\"0\"\n",
    ")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667f4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict( \n",
    "\n",
    "what_applicant_did_for_each_section={\n",
    "    \"categorical encoding\" : 'binary encoding',\n",
    "},\n",
    "\n",
    "objectives_and_constraints = [\n",
    "    \"Model must be explainable.\"\n",
    "],\n",
    "\n",
    "insights = [],\n",
    "\n",
    "current_section_chat=\"\"\"Interviewer: Why did you use Binary Encoding over One Hot Encoding on the categorical variables?\n",
    "Applicant: Because binary encoding would require fewer columns to encode the categorical columns than one hot encoding, which would enable my tree-based model to perform better.\n",
    "Interviewer: The problem with using a model trained on Binary-Encoded categorical variables is that it would be less explainable than one trained using One Hot-Encoded ones, since it is harder to interpret the effect each column in the binary vector has on the model output. Do you still think Binary Encoding is the optimal choice? \n",
    "Applicant: That's true. No - I think One Hot Encoding is the optimal choice.\"\"\",\n",
    "\n",
    "is_completion_correct=1\n",
    ")\n",
    "    \n",
    "completion_args = dict(\n",
    "validation_of_response=\"1\",\n",
    "\n",
    "applicant_justifications=\"\"\"NA\"\"\",\n",
    "\n",
    "num_correct_subpoints=\"0\", \n",
    "num_incorrect_subpoints=\"0\", \n",
    "num_irrelevant_subpoints=\"0\"\n",
    ")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e8e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict( \n",
    "\n",
    "what_applicant_did_for_each_section={\n",
    "    \"categorical encoding\" : 'binary encoding',\n",
    "    \"dealing with class imbalance\" : \"Downsample the majority class\",\n",
    "},\n",
    "\n",
    "objectives_and_constraints = [\n",
    "    \"Model must be explainable.\",\n",
    "    \"Model must be time-efficient (should be quick to train).\"\n",
    "],\n",
    "\n",
    "insights = [],\n",
    "\n",
    "current_section_chat=\"\"\"Interviewer: Why did you downsample the majority class instead of upsampling the minority class?\n",
    "Applicant: By downsampling, I could reduce the number of observations in the dataset (while at the same time not loosing too much useful information, since the imbalance is so high), which would help in training the model quicker.\n",
    "Interviewer: Downsampling the majority class would lead to a loss of information, and therefore a less performant model. Do you still think it is the optimal choice? \n",
    "Applicant: The loss of observations is acceptable to me, since one of the requirements for my solution is that the model must be time-efficient. By training on fewer observations, it can be trained faster. So yes, I still think downsampling is the right choice.\"\"\",\n",
    "\n",
    "is_completion_correct=1\n",
    ")\n",
    "    \n",
    "completion_args = dict(\n",
    "validation_of_response=\"1\",\n",
    "\n",
    "applicant_justifications=\"\"\"Subpoint: <Downsampling reduces the dataset size, allowing the model to be trained faster.><1><NA>\"\"\",\n",
    "\n",
    "num_correct_subpoints=\"1\", \n",
    "num_incorrect_subpoints=\"0\", \n",
    "num_irrelevant_subpoints=\"0\"\n",
    ")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ab7a492",
   "metadata": {},
   "source": [
    "# ask_devils_advocate_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9d8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ask_devils_advocate_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb28ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version_metadata = dict(\n",
    "\n",
    "prompt_template = \\\n",
    "\"\"\"What applicant did for each section:\n",
    "{what_applicant_did_for_each_section}\n",
    "\n",
    "Objectives/Constraints:\n",
    "{objectives_and_constraints}\n",
    "\n",
    "Insights:\n",
    "{insights}\n",
    "\n",
    "Current section chat:\n",
    "{current_section_chat}\n",
    "\n",
    "Details:\n",
    "Is question correct: {is_completion_correct}\n",
    "###\n",
    "\"\"\",\n",
    "\n",
    "completion_template = \\\n",
    "\"\"\"Subject 1: {subject_1}\n",
    "Subject 2: {subject_2}\n",
    "Disadvantage of using Subject 1: {disadvantage_of_using_subject_1}\n",
    "Interviewer: {interviewer_dialogue}\n",
    "\"\"\",\n",
    "\n",
    "nested_completion_templates = None,\n",
    "\n",
    "kshot_header = \\\n",
    "\"\"\"Interviewer is interviewing Applicant for a job as a Data Scientist.\n",
    "Interviewer asks Applicant a question similar to \"But the issue with your approach is <>. Do you still think it is the optimal choice?\" in the following format:\n",
    "\n",
    "What applicant did for each section:\n",
    "$<a summary of the techniques the applicant has used for each section of the interview>\n",
    "\n",
    "Objectives/Constraints:\n",
    "$<the objective of the case study, and any constraints/requirements to applicant's solution>\n",
    "\n",
    "Insights:\n",
    "$<any insights about the problem>\n",
    "\n",
    "Current section chat:\n",
    "$<Conversation so far between Interviewer and Applicant>\n",
    "\n",
    "Details:\n",
    "Is validation correct: $<1 or 0 - whether Interviewer's validation of applicant's response is correct or not>\n",
    "###\n",
    "Subject 1: $<approach applicant chose to use>\n",
    "Subject 2: $<approach applicant considered using, but decided not to. If applicant didn't consider any other approaches, this should be 'NA'>\n",
    "Disadvantage of using Subject 1: $<a disadvantage/negative implication of using the applicant's chosen approach, given the objectives/constrains of the problem and any insights we have about the data. This must be something that has not been mentioned already in the conversation.>\n",
    "Interviewer: $<Interviewer's dialogue on the distadvantage of using the applicant's chosen approach, and whether the applicant still thinks their approach is optimal>\n",
    "\n",
    "Below are some correct examples:\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "finetuned_model_name = None,\n",
    "\n",
    "stop_sequence = \"$$$\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5704b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_model_version(model_name, new_model_version_metadata, set_as_best_model_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fbf10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata, model_version = get_model_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d8f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict( \n",
    "\n",
    "    what_applicant_did_for_each_section={\n",
    "        \"categorical encoding\" : 'binary encoding',\n",
    "        \"dealing with class imbalance\" : \"Downsample the majority class\",\n",
    "    },\n",
    "\n",
    "    objectives_and_constraints = [\n",
    "        \"Model must be explainable.\",\n",
    "        \"Model must be time-efficient (should be quick to train).\"\n",
    "    ],\n",
    "\n",
    "    insights = [],\n",
    "\n",
    "    current_section_chat=\"\"\"Interviewer: Why did you downsample the majority class instead of upsampling the minority class?\n",
    "    Applicant: By downsampling, I could reduce the number of observations in the dataset (while at the same time not loosing too much useful information, since the imbalance is so high), which would help in training the model quicker.\"\"\",\n",
    "\n",
    "    is_completion_correct=1\n",
    ")\n",
    "\n",
    "##\n",
    "completion_args = dict(\n",
    "    subject_1 = \"Downsample the majority class\",\n",
    "    subject_2 = \"Upsample the minority class\",\n",
    "    disadvantage_of_using_subject_1 = \"Downsampling reduces the number of observations for the model to learn from.\",\n",
    "    interviewer_dialogue = \"Downsampling the majority class would lead to a loss of information, and therefore a less performant model. Do you still think it is the optimal choice?\"\n",
    ")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "039b87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_args = dict( \n",
    "\n",
    "what_applicant_did_for_each_section={\n",
    "    \"algorithm selection\" : \"Linear Regression\",\n",
    "    \"categorical encoding\" : 'binary encoding',\n",
    "    \"dealing with class imbalance\" : \"Downsample the majority class\",\n",
    "},\n",
    "\n",
    "objectives_and_constraints = [\n",
    "    \"Model must have high performance.\",\n",
    "],\n",
    "\n",
    "insights = [\n",
    "    \"Some numerical columns in the dataset are collinear.\",\n",
    "    \"There is a lienar relationship between the independent and dependent variables.\"\n",
    "],\n",
    "\n",
    "current_section_chat=\"\"\"Interviewer: Why did you use a Linear Regression model instead of a Random Forest?\n",
    "Applicant: The data was mostly linear, so I used a Linear Regression model because it is a simple model and can learn linear relationships. I didn't want to overcomplicate the matter by fitting a complex model like Random Forest.\"\"\",\n",
    "\n",
    "is_completion_correct=1\n",
    ")\n",
    "\n",
    "##\n",
    "completion_args = dict(\n",
    "    subject_1 = \"Linear Regression\",\n",
    "    subject_2 = \"Random Forest\",\n",
    "    disadvantage_of_using_subject_1 = \"Linear Regression performs poorly when there is multicollinearity in the data.\",\n",
    "    interviewer_dialogue = \"But some of the columns in the dataset are collinear. Do you still think Linear Regression is the optimal choice for a performant model?\"\n",
    ")\n",
    "\n",
    "observation_prompt = model_metadata['prompt_template'].format(**prompt_args)\n",
    "completion = model_metadata['completion_template'].format(**completion_args)\n",
    "observation_details = dict(model_name=model_name, \n",
    "                               model_version=model_version,\n",
    "                               prompt_template=model_metadata['prompt_template'], \n",
    "                               completion_template=model_metadata['completion_template'], \n",
    "                               prompt_args=prompt_args, \n",
    "                               completion_args=completion_args,\n",
    "                               prompt = observation_prompt,\n",
    "                               completion=completion)\n",
    "\n",
    "prepare_data.add_observation_to_finetuning_datasets(observation_details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_interview_chatbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "41f12de9af4222e4b257cc56cd09168715a90beb6fcd186c29ab659bfe87472a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
